{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fcd6c3",
   "metadata": {},
   "source": [
    "# New generation datamodels and DBMSS Project\n",
    "2023 / april 2025 edition\n",
    "\n",
    "This notebook has been developed in accordance with the project guidelines provided by the professor. You can consult the guidelines at the following link: [Project Guidelines](assets/Project2023-vers1.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e01d1-ffc7-4f78-bf43-acbbe19cb409",
   "metadata": {},
   "source": [
    "## 1) Transaction Data Simulator Tool\n",
    "\n",
    "This section focuses on how the various provided scripts were combined to create a single versatile script that, through the use of parameters, is capable of generating CSV files containing all the data to be inserted into the database. We will not explain the functionality of the Python scripts or the meaning of the data generated by the tool, as these aspects are clearly detailed on the [linked page](https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_3_GettingStarted/SimulatedDataset.html).\n",
    "\n",
    "To proceed, the following Python packages and Python sources (from this project's repository) are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5306d3-a84f-42dd-ab03-e115c192b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../GenerationScript/Transaction_data_simulator_code'))\n",
    "from add_frauds import add_frauds\n",
    "from generate_dataset import generate_dataset\n",
    "\n",
    "pd.set_option('display.max_rows', 10) \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab3f3b",
   "metadata": {},
   "source": [
    "### 1.1) Parameters\n",
    "\n",
    "To manage the parameters for the script in a simple way, I decided to use an array of objects. Each object represents the entire configuration for creating a single database, allowing the script to create multiple databases with different characteristics and data volumes in one run.\n",
    "\n",
    "Each object in the array, so each database configuration, contains:\n",
    "- DB_name: The name of the database.\n",
    "- n_customers: The number of customers to create.\n",
    "- n_terminals: The number of terminals to create.\n",
    "- start_date: The start date for generating transaction data.\n",
    "- n_days: The number of days after the start_date to use for generating transaction data.\n",
    "- radius: The action radius for customers. A customer can only perform transactions at a terminal within their radius.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d623f6c9",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "DBs = [\n",
    "   {\n",
    "       \"DB_name\": \"DB-410KB\",\n",
    "       \"n_customers\": 500,\n",
    "       \"n_terminals\": 300,\n",
    "       \"n_days\": 7,\n",
    "       \"start_date\": '2024-12-30',\n",
    "       \"radius\": 10\n",
    "    },\n",
    "    {\n",
    "        \"DB_name\": \"DB-14MB\",\n",
    "        \"n_customers\": 200,\n",
    "        \"n_terminals\": 50,\n",
    "        \"n_days\": 700,\n",
    "        \"start_date\": '2022-01-01',\n",
    "        \"radius\": 15\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e01739-8ad8-4b0d-b4c4-479deec0a9e2",
   "metadata": {},
   "source": [
    "### 1.2) Generation Script\n",
    "\n",
    "Below is the commented code for generating the databases using the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4744055c-ee97-4bd0-a459-6dd87dbbe7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate customer profiles table: 0.00s\n",
      "Time to generate terminal profiles table: 0.00s\n",
      "Time to associate terminals to customers: 0.05s\n",
      "Time to generate transactions: 0.40s\n",
      "Number of frauds from scenario 1: 1\n",
      "Number of frauds from scenario 2: 127\n",
      "Number of frauds from scenario 3: 46\n",
      "Database data saved in: /mnt/1364D0FF74AFABFF/unimi/new generation/progetto/NewGenerationDBMSSProject/Generated_DBs/DB-410KB/\n",
      "\n",
      "Time to generate customer profiles table: 0.00s\n",
      "Time to generate terminal profiles table: 0.00s\n",
      "Time to associate terminals to customers: 0.02s\n",
      "Time to generate transactions: 4.26s\n",
      "Number of frauds from scenario 1: 160\n",
      "Number of frauds from scenario 2: 177216\n",
      "Number of frauds from scenario 3: 5540\n",
      "Database data saved in: /mnt/1364D0FF74AFABFF/unimi/new generation/progetto/NewGenerationDBMSSProject/Generated_DBs/DB-14MB/\n",
      "\n",
      "DONE! All DBs have been created\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"\"\n",
    "# Loop through the databases defined in the configuration file\n",
    "for db in DBs:\n",
    "    # Generate database tables using configuration values\n",
    "    (customer_profiles_table, terminal_profiles_table, transactions_df) = generate_dataset(\n",
    "        n_customers=db[\"n_customers\"], \n",
    "        n_terminals=db[\"n_terminals\"], \n",
    "        nb_days=db[\"n_days\"], \n",
    "        start_date=db[\"start_date\"], \n",
    "        r=db[\"radius\"]\n",
    "    )\n",
    "\n",
    "    # Add fraud data to the transactions\n",
    "    transactions_df = add_frauds(customer_profiles_table, terminal_profiles_table, transactions_df)\n",
    "\n",
    "    \n",
    "    # Convert the values of the 'available_terminals' series, as the integers in the list are numpy integers\n",
    "    customer_profiles_table['available_terminals'] = customer_profiles_table['available_terminals'].apply(\n",
    "        lambda lst: [int(i) if isinstance(i, np.integer) else i for i in lst] if isinstance(lst, (list, np.array)) else lst\n",
    "    )\n",
    "\n",
    "    # Prepare for saving the database\n",
    "    output_dir = os.path.join(os.getcwd(), '..', 'Generated_DBs', db[\"DB_name\"])\n",
    "\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Saving customers\n",
    "    customer_profiles_table.to_csv(output_dir + '/customers.csv', sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    # Saving terminals\n",
    "    terminal_profiles_table.to_csv(output_dir + '/terminals.csv', sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    # Saving transactions\n",
    "    transactions_df.to_csv(output_dir + '/transactions.csv', sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    print(f\"Database data saved in: {os.path.abspath(output_dir)}/\\n\")\n",
    "\n",
    "\n",
    "print(\"DONE! All DBs have been created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7664f-110a-483a-b745-cf45712a853d",
   "metadata": {},
   "source": [
    "### 1.3) Generated CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48565d8-453d-4bc9-a6a1-219b0c0fc01c",
   "metadata": {},
   "source": [
    "#### Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb212b5-92c1-46e0-ada7-dcdf9339fade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_customer_id</th>\n",
       "      <th>y_customer_id</th>\n",
       "      <th>mean_amount</th>\n",
       "      <th>std_amount</th>\n",
       "      <th>mean_nb_tx_per_day</th>\n",
       "      <th>available_terminals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.881350</td>\n",
       "      <td>71.518937</td>\n",
       "      <td>62.262521</td>\n",
       "      <td>31.131260</td>\n",
       "      <td>2.179533</td>\n",
       "      <td>[0, 5, 29, 44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.365480</td>\n",
       "      <td>64.589411</td>\n",
       "      <td>46.570785</td>\n",
       "      <td>23.285393</td>\n",
       "      <td>3.567092</td>\n",
       "      <td>[0, 4, 5, 8, 11, 46]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.366276</td>\n",
       "      <td>38.344152</td>\n",
       "      <td>80.213879</td>\n",
       "      <td>40.106939</td>\n",
       "      <td>2.115580</td>\n",
       "      <td>[16, 23, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.804456</td>\n",
       "      <td>92.559664</td>\n",
       "      <td>11.748426</td>\n",
       "      <td>5.874213</td>\n",
       "      <td>0.348517</td>\n",
       "      <td>[18, 43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.021840</td>\n",
       "      <td>83.261985</td>\n",
       "      <td>78.924891</td>\n",
       "      <td>39.462446</td>\n",
       "      <td>3.480049</td>\n",
       "      <td>[19, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>13.907270</td>\n",
       "      <td>42.690436</td>\n",
       "      <td>85.071214</td>\n",
       "      <td>42.535607</td>\n",
       "      <td>3.272133</td>\n",
       "      <td>[3, 15, 22, 30, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>10.241376</td>\n",
       "      <td>15.638335</td>\n",
       "      <td>33.898876</td>\n",
       "      <td>16.949438</td>\n",
       "      <td>0.301436</td>\n",
       "      <td>[2, 9, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>42.466300</td>\n",
       "      <td>10.761771</td>\n",
       "      <td>58.980671</td>\n",
       "      <td>29.490336</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>[24, 27, 37, 47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>59.643307</td>\n",
       "      <td>11.752564</td>\n",
       "      <td>97.708967</td>\n",
       "      <td>48.854484</td>\n",
       "      <td>3.730245</td>\n",
       "      <td>[27, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>39.179694</td>\n",
       "      <td>24.217859</td>\n",
       "      <td>28.787830</td>\n",
       "      <td>14.393915</td>\n",
       "      <td>1.933574</td>\n",
       "      <td>[37, 47]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_customer_id  y_customer_id  mean_amount  std_amount  \\\n",
       "CUSTOMER_ID                                                          \n",
       "0                54.881350      71.518937    62.262521   31.131260   \n",
       "1                42.365480      64.589411    46.570785   23.285393   \n",
       "2                96.366276      38.344152    80.213879   40.106939   \n",
       "3                56.804456      92.559664    11.748426    5.874213   \n",
       "4                 2.021840      83.261985    78.924891   39.462446   \n",
       "...                    ...            ...          ...         ...   \n",
       "195              13.907270      42.690436    85.071214   42.535607   \n",
       "196              10.241376      15.638335    33.898876   16.949438   \n",
       "197              42.466300      10.761771    58.980671   29.490336   \n",
       "198              59.643307      11.752564    97.708967   48.854484   \n",
       "199              39.179694      24.217859    28.787830   14.393915   \n",
       "\n",
       "             mean_nb_tx_per_day   available_terminals  \n",
       "CUSTOMER_ID                                            \n",
       "0                      2.179533        [0, 5, 29, 44]  \n",
       "1                      3.567092  [0, 4, 5, 8, 11, 46]  \n",
       "2                      2.115580          [16, 23, 38]  \n",
       "3                      0.348517              [18, 43]  \n",
       "4                      3.480049              [19, 36]  \n",
       "...                         ...                   ...  \n",
       "195                    3.272133   [3, 15, 22, 30, 32]  \n",
       "196                    0.301436            [2, 9, 13]  \n",
       "197                    0.986228      [24, 27, 37, 47]  \n",
       "198                    3.730245              [27, 28]  \n",
       "199                    1.933574              [37, 47]  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(output_dir, 'customers.csv'), sep=';', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29e7a5-d9b6-4e4b-8ef4-54d84a9ed8ee",
   "metadata": {},
   "source": [
    "#### Terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfd3649-9afb-4440-9863-4fb1eadeba66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_terminal_id</th>\n",
       "      <th>y_terminal_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.702200</td>\n",
       "      <td>72.032449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011437</td>\n",
       "      <td>30.233257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.675589</td>\n",
       "      <td>9.233859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.626021</td>\n",
       "      <td>34.556073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.676747</td>\n",
       "      <td>53.881673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>11.474597</td>\n",
       "      <td>94.948926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>44.991213</td>\n",
       "      <td>57.838961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>40.813680</td>\n",
       "      <td>23.702698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>90.337952</td>\n",
       "      <td>57.367949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.287033</td>\n",
       "      <td>61.714491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_terminal_id  y_terminal_id\n",
       "TERMINAL_ID                              \n",
       "0                41.702200      72.032449\n",
       "1                 0.011437      30.233257\n",
       "2                14.675589       9.233859\n",
       "3                18.626021      34.556073\n",
       "4                39.676747      53.881673\n",
       "...                    ...            ...\n",
       "45               11.474597      94.948926\n",
       "46               44.991213      57.838961\n",
       "47               40.813680      23.702698\n",
       "48               90.337952      57.367949\n",
       "49                0.287033      61.714491\n",
       "\n",
       "[50 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(output_dir, 'terminals.csv'), sep=';', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338c7ff-e76e-4a56-a4fd-daae427cb364",
   "metadata": {},
   "source": [
    "#### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2384961-3d3b-40d4-b7c3-7e0a713ab315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_TIME_SECONDS</th>\n",
       "      <th>TX_TIME_DAYS</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_FRAUD_SCENARIO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:07:56</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>146.00</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 00:32:35</td>\n",
       "      <td>183</td>\n",
       "      <td>47</td>\n",
       "      <td>39.30</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 01:11:00</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 01:56:44</td>\n",
       "      <td>55</td>\n",
       "      <td>18</td>\n",
       "      <td>35.06</td>\n",
       "      <td>7004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 01:59:15</td>\n",
       "      <td>159</td>\n",
       "      <td>9</td>\n",
       "      <td>54.22</td>\n",
       "      <td>7155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262558</th>\n",
       "      <td>2023-12-01 22:34:42</td>\n",
       "      <td>57</td>\n",
       "      <td>40</td>\n",
       "      <td>21.72</td>\n",
       "      <td>60474882</td>\n",
       "      <td>699</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262559</th>\n",
       "      <td>2023-12-01 22:45:52</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>161.55</td>\n",
       "      <td>60475552</td>\n",
       "      <td>699</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262560</th>\n",
       "      <td>2023-12-01 22:47:16</td>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>9.64</td>\n",
       "      <td>60475636</td>\n",
       "      <td>699</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262561</th>\n",
       "      <td>2023-12-01 22:59:15</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>38.33</td>\n",
       "      <td>60476355</td>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262562</th>\n",
       "      <td>2023-12-01 23:07:15</td>\n",
       "      <td>115</td>\n",
       "      <td>26</td>\n",
       "      <td>43.46</td>\n",
       "      <td>60476835</td>\n",
       "      <td>699</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262563 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  \\\n",
       "TRANSACTION_ID                                                             \n",
       "0               2022-01-01 00:07:56            2           16     146.00   \n",
       "1               2022-01-01 00:32:35          183           47      39.30   \n",
       "2               2022-01-01 01:11:00            8            5       2.08   \n",
       "3               2022-01-01 01:56:44           55           18      35.06   \n",
       "4               2022-01-01 01:59:15          159            9      54.22   \n",
       "...                             ...          ...          ...        ...   \n",
       "262558          2023-12-01 22:34:42           57           40      21.72   \n",
       "262559          2023-12-01 22:45:52            9           33     161.55   \n",
       "262560          2023-12-01 22:47:16           41           20       9.64   \n",
       "262561          2023-12-01 22:59:15            1           46      38.33   \n",
       "262562          2023-12-01 23:07:15          115           26      43.46   \n",
       "\n",
       "                TX_TIME_SECONDS  TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n",
       "TRANSACTION_ID                                                              \n",
       "0                           476             0         0                  0  \n",
       "1                          1955             0         0                  0  \n",
       "2                          4260             0         0                  0  \n",
       "3                          7004             0         0                  0  \n",
       "4                          7155             0         0                  0  \n",
       "...                         ...           ...       ...                ...  \n",
       "262558                 60474882           699         1                  2  \n",
       "262559                 60475552           699         1                  2  \n",
       "262560                 60475636           699         1                  2  \n",
       "262561                 60476355           699         0                  0  \n",
       "262562                 60476835           699         1                  2  \n",
       "\n",
       "[262563 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(output_dir, 'transactions.csv'), sep=';', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8392b-ce13-4d86-b5c7-d6d12aabc795",
   "metadata": {},
   "source": [
    "### 1.4) Generated DBs\n",
    "The project guidelines require three databases to be generated with sizes of 50 MB, 100 MB, and 200 MB. The database generation script does not allow you to directly specify the desired database size. Instead, all of the previously identified parameters must be specified. After several tests, I determined the parameters needed to generate the three databases of the desired sizes.\n",
    "\n",
    "It is important to note that the generated databases simulate scenarios with a high transaction volume and a limited number of customers and terminals. This feature reflects a worst-case scenario for our workload, which should be taken into account when evaluating performance.\n",
    "\n",
    "Unfortunately, none of the three databases requested by the project can be loaded on a free Neo4j Aura instance due to the excessive number of relationships, which exceeds the 400K limit. So for the demonstration purposes of this notebook, and to ensure that the provided code can run without requiring a paid Neo4j instance, I decided to use a 14MB database that we had previously generated with a free Neo4j Aura instance that I had created. Obviously since the free version goes offline after a period of inactivity you can substitute in the code I have prepared in section 4 by entering link and credentials of your free instance.\n",
    "\n",
    "Despite the performance limitation in the last section, the queries run in this notebook will also be applied to 50MB, 100MB, and 200MB databases, but on a local instance that doesn't have any limitations.\n",
    "\n",
    "Since creating these databases is time-consuming, I will not run the database creation script during this demonstration. However, the script can be used to generate them if desired, below are the parameters to generate the desired databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e98635-503b-4cd9-860f-451457963a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBs = [\n",
    "    {\n",
    "        \"DB_name\": \"50MB\",\n",
    "        \"n_customers\": 1000,\n",
    "        \"n_terminals\": 500,\n",
    "        \"n_days\": 500,\n",
    "        \"start_date\": '2022-01-01',\n",
    "        \"radius\": 5\n",
    "    },\n",
    "    {\n",
    "        \"DB_name\": \"100MB\",\n",
    "        \"n_customers\": 1200,\n",
    "        \"n_terminals\": 600,\n",
    "        \"n_days\": 800,\n",
    "        \"start_date\": '2022-01-01',\n",
    "        \"radius\": 5\n",
    "    },\n",
    "    {\n",
    "        \"DB_name\": \"200MB\",\n",
    "        \"n_customers\": 2000,\n",
    "        \"n_terminals\": 1000,\n",
    "        \"n_days\": 900,\n",
    "        \"start_date\": '2022-01-01',\n",
    "        \"radius\": 5\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd624c-46c3-4f07-a8b8-f3e58796b4f2",
   "metadata": {},
   "source": [
    "## 2) Conceptual Model\n",
    "\n",
    "To create the following conceptual model, I analyzed the CSV files generated by the *Transaction Data Simulator* tool. This analysis allowed me to understand the semantics of the data and to design a clear and simple structure that illustrates the relationships between the data to be stored in the database.\n",
    "\n",
    "### 2.1) UML Class Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7225c-f30e-4129-aa42-5f9837bd76d4",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Conceptual model UML.svg\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9b0af-a1db-4aad-b8ce-ab071041fd14",
   "metadata": {},
   "source": [
    "### 2.2) Costraints\n",
    "#### Terminal\n",
    "- 0 <= `coords.x` <= 100\n",
    "- 0 <= `coords.y` <= 100\n",
    "\n",
    "#### Customer\n",
    "- 0 <= `coords.x` <= 100\n",
    "- 0 <= `coords.y` <= 100\n",
    "- `spending_mean` >= 0\n",
    "- `spending_std` >= 0\n",
    "- `transactions_per_day_mean` >= 0\n",
    "\n",
    "#### Transactions\n",
    "- `amount` > 0\n",
    "- 0 <= `fraud_scenario` <= 3\n",
    "- 0 <= `security_feeling` <= 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cadaf-07f9-4964-8509-e3bed43d7c1c",
   "metadata": {},
   "source": [
    "## 3) Logical Model\n",
    "\n",
    "Before proceeding with the logical model, it is important to indicate which database I have chosen to manage the data and what decisions I have made about how to represent the data to meet the workload requirements.\n",
    "\n",
    "### 3.1) Database\n",
    "I chose Neo4j as the database because the nature of the data suggests a graph structure. In fact, all the relationships present are of the N:N type and such relationships are well handled by graph databases. \n",
    "\n",
    "Furthermore, this choice was confirmed by the workload, in particular by query 3C, which involves continuous traversal of the relationships up to a certain `K` value that determines when to stop. Executing this query would be extremely costly if we had to perform a join (or lookup) for each relationship traversed. \n",
    "\n",
    "In addition, as we will see later, Cypher, Neo4j's query language, provides a library called APOC that allows us to execute query 3C with impressive performance.\n",
    "\n",
    "### 3.2) Data representation (workload friendly)\n",
    "Since Neo4j does not allow the definition of custom types or the insertion of objects within node properties, I decided to eliminate all custom types and implement them using primitive types. For the custom types representing objects, I created a property for each attribute with its corresponding primitive type. For enums, I used simple strings.\n",
    "\n",
    "The attribute names in the logical model differ from those in the conceptual model because they are based on those used by the *Transaction Data Simulator* tool. The meaning of any ambiguous or newly introduced fields can be determined by:  \n",
    "- Referring to the *Transaction Data Simulator* tool documentation for fields generated by the tool.  \n",
    "- Reading the following section, which explains the new fields I have added.  \n",
    "- Consulting the project guidelines, which detail and justify the fields explicitly required in the extended database.  \n",
    "\n",
    "As we will see later, in order to improve the efficiency of the indexing workload, I decided to split the `transactions.registration` field into its components: day, month, year, and time. These components are now represented as `tx_date_day`, `tx_date_month`, `tx_date_year` and `tx_date_time` respectively. This division was made because many queries in the workload filter data using only the month and year of the `transactions.registration` field. If I had created an index on the entire field, it would not have been used because the filters in the queries would only use a subset of the entire field. Therefore, the division was made and a composite index was created only on the year and month fields.\n",
    "\n",
    "The data types specified are those that exist in Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff40efb-95e5-4285-a131-1b0e61bf7a93",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Logical model UML.svg\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c04e6-2e07-463d-af89-5863a1ed487a",
   "metadata": {},
   "source": [
    "### 3.3) Costraints\n",
    "#### Terminal\n",
    "- 0 <= `x_terminal_id` <= 100\n",
    "- 0 <= `y_terminal_id` <= 100\n",
    "\n",
    "#### Customer\n",
    "- 0 <= `x_customer_id` <= 100\n",
    "- 0 <= `y_customer_id` <= 100\n",
    "- `mean_amount` >= 0\n",
    "- `std_amount` >= 0\n",
    "- `mean_nb_tx_per_day` >= 0\n",
    "\n",
    "#### Transactions\n",
    "- `tx_amount` > 0\n",
    "- 0 <= `tx_fraud_scenario` <= 3\n",
    "- 0 <= `tx_security_feeling` <= 5\n",
    "- `tx_date_day`, `tx_date_month`, `tx_date_year` form a correct date type object \n",
    "- `tx_date_time` forms a correct localTime object\n",
    "- `tx_day_period` is one of the following strings [\"morning\", \"afternoon\", \"evening\", \"night\"]\n",
    "- `tx_products_type` is one of the following strings [\"high-tech\", \"food\", \"clothing\", \"consumable\", \"other\"]\n",
    "\n",
    "### 3.4) Assumptions\n",
    "Since the constraints that can be implemented in Neo4j focus only on the structure and data type, and do not allow constraints on the actual values or the direction of relationships, I assume that whatever software is providing the data to be inserted into the database has correctly implemented all the constraints listed above (except for the constraints on the `tx_date_...` properties, since these can be validated at the database level). In our case, we assume that the values produced by the *Transaction Data Simulator* tool are correct and satisfy the constraints. \n",
    "\n",
    "Since Neo4j constraints also do not allow us to define the direction of relationships, it is our responsibility to ensure that we do not make mistakes in the queries we use to create relationships, and to avoid creating relationships in the wrong direction.\n",
    "\n",
    "For more detailed information, I refer you to the Neo4j [documentation](https://neo4j.com/docs/cypher-manual/current/constraints/managing-constraints/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39af186-d0a2-4bf5-b597-b8f5a350bc51",
   "metadata": {},
   "source": [
    "## 4) Neo4j Data Loading\n",
    "To proceed the following Python packages are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95383e31-f7c8-4b24-a4e9-0facf81aecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import neo4j\n",
    "import logging\n",
    "logging.getLogger(\"neo4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7822175-8e8e-4ac4-93da-66f667091953",
   "metadata": {},
   "source": [
    "To facilitate interactions with Neo4j, we will define some \"kernel\" functions that will be used to interface with the database. These functions will simplify data management with Neo4j and provide reusable methods for the rest of the project.\n",
    "\n",
    "To keep the code simple and easy to understand, the \"kernel\" functions will be passed queries with parameters embedded directly through string concatenation. While this approach allows for simpler coding, it exposes potential vulnerabilities related to direct parameter concatenation in queries. Since addressing these security concerns is not the goal of this project, but rather demonstrating how the database was managed to optimize workload, I chose to keep the code as simple as possible.\n",
    "\n",
    "Before defining the kernel functions, we set some configuration parameters that will be useful not only for the kernel functions themselves, but also for the various queries that will be executed by the kernel functions later in the project.\n",
    "Among the configuration parameters we have:\n",
    "- `customers_csv_link`, `terminals_csv_link`, `transactions_csv_link`: These parameters refer to the CSV files generated for the 14MB database. They can be either local file paths or network links. A separate section will explain why network links are preferred in this case. Additionally, in the performance analysis section, we will include the database load times for the 50MB, 100MB, and 200MB databases to provide a comprehensive comparison.\n",
    "  \n",
    "- `lines_per_commit_call` and `lines_per_commit_apoc`: these parameters are used to define the number of operations included in a single batch, where the changes on the DB are committed after each batch. I have defined 2 different parameters because, in order to maximise performance, the batch size depends on how the job is defined. Jobs using Cypher `CALL {} IN TRANSACTIONS OF ... ROWS` will generally allow larger batch sizes than those defined with the `APOC' library.ROWS\n",
    "  \n",
    "- `parallel_loading`: useful for the batch operations mentioned in the previous point. This parameter indicates whether the database should perform the batch operations in parallel or sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c58278ee-178d-4b60-b1ff-88abd5045d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parameters\n",
    "config = {\n",
    "    \"customers_csv_link\":  \"https://www.dropbox.com/scl/fi/ofi4fd99aydhnp30i2spy/customers.csv?rlkey=iqfr9uaty48gc4toxlssqcvf1&st=h3vqznsz&dl=1\",\n",
    "    \"terminals_csv_link\":  \"https://www.dropbox.com/scl/fi/4tt3cyhnpj4q3y49xksrp/terminals.csv?rlkey=1881everw81e38nc0xa2n32ct&st=8eurat39&dl=1\",\n",
    "    \"transactions_csv_link\":  \"https://www.dropbox.com/scl/fi/we51epibb3p98syq67kcq/transactions.csv?rlkey=4bm84xkt9b7rub9rs0u7cough&st=j1xhtfsa&dl=1\",\n",
    "    \"lines_per_commit_call\": 100000,\n",
    "    \"lines_per_commit_apoc\": 10000,\n",
    "    \"parallel_loading\": \"true\"\n",
    "}\n",
    "\n",
    "def get_neo4j_connection():\n",
    "    try:\n",
    "        #Using environment variables (recommended): This method securely stores credentials outside the code by using environment variables.\n",
    "        #uri = os.getenv('NEO4J_URI')\n",
    "        #user = os.getenv('NEO4J_USERNAME')\n",
    "        #password = os.getenv('NEO4J_PASSWORD')\n",
    "        \n",
    "        #Using plain strings (not recommended): This method directly includes credentials in the code, which exposes them to potential security risks.\n",
    "        #In this case, to keep things as simple as possible, I will use plain text credentials since they are for a free version of Neo4j.\n",
    "        #You can create it by following this link: https://neo4j.com/product/auradb\n",
    "        uri = \"neo4j+s://45d4bc57.databases.neo4j.io\"\n",
    "        user = \"neo4j\"\n",
    "        password = \"o8mbh0hFGILahScLJw2yTYWIwQ6z7lPhQT6m-U2W1c8\"\n",
    "\n",
    "        #local db\n",
    "        #uri = \"bolt://localhost:7687\"\n",
    "        #user = \"neo4j\"\n",
    "        #password = \"abcdefgh\"\n",
    "\n",
    "        return neo4j.GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred while connecting to Neo4j: {e}\")\n",
    "        return None\n",
    "\n",
    "def close_neo4j_connection(driver):\n",
    "    if driver is not None:\n",
    "        driver.close()\n",
    "\n",
    "def clear_database():\n",
    "    driver = get_neo4j_connection()\n",
    "    delete_nodes_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        CALL apoc.nodes.delete(n, $lines_per_commit_apoc) YIELD value\n",
    "        RETURN value\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        with driver.session() as session:\n",
    "            session.run(delete_nodes_query, {\"lines_per_commit_apoc\": config[\"lines_per_commit_apoc\"]})\n",
    "\n",
    "            constraints_result = session.run(\"SHOW CONSTRAINTS\")\n",
    "            for record in constraints_result:\n",
    "                drop_constraint_query = \"DROP CONSTRAINT $name\"\n",
    "                session.run(drop_constraint_query, {\"name\": record[\"name\"]})\n",
    "\n",
    "            indexes_result = session.run(\"SHOW INDEXES\")\n",
    "            for record in indexes_result:\n",
    "                drop_index_query = \"DROP INDEX $name\"\n",
    "                session.run(drop_index_query, {\"name\": record[\"name\"]})\n",
    "\n",
    "            print(\"clear_database execution time: {:.2f}s\".format(time.time() - start_time))\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR clear_database: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)\n",
    "\n",
    "def execute_query_commands(name, queries):\n",
    "    driver = get_neo4j_connection()\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            start_time = time.time()\n",
    "            for query in queries:\n",
    "                try:\n",
    "                    session.run(query)\n",
    "                except Exception as e:\n",
    "                    return False\n",
    "            \n",
    "        print(f\"{name} execution time: {{:.2f}}s\".format(time.time() - start_time))\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR {name}: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)\n",
    "\n",
    "def execute_query_df(name, query):\n",
    "    driver = get_neo4j_connection()\n",
    "    if driver is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        start_time=time.time()\n",
    "        result = driver.execute_query(query, result_transformer_= neo4j.Result.to_df)\n",
    "        print(f\"{name} execution time: {{:.2f}}s\".format(time.time() - start_time))\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR {name}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098dadb5-4648-420c-b573-ab23124907dc",
   "metadata": {},
   "source": [
    "This step is unnecessary if you have just created a new database instance, but **if you are reusing an instance on which you have already performed some operations**, such as running this notebook, **it is necessary to restore it to its original state** by clearing everything. This is where the `clear_database()` function comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ad3ab8-2b74-46dd-a5ae-f02d0092de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear_database execution time: 25.96s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893ebdd-a7d9-4b76-912e-0923b67c4c4f",
   "metadata": {},
   "source": [
    "### 4.1) Schema\n",
    "Neo4j's constraints focus solely on data structure, as they are used to define a schema for the data. The schemaless nature of Neo4j, or the schemaless nature of NoSQL databases in general, allows data to be inserted with maximum flexibility without the need to define a formal schema in advance. This flexibility allows for handling heterogeneous data and adapting to changes over time, making it ideal for scenarios where the data structure may evolve.\n",
    "\n",
    "Despite this flexibility, defining a schema is still considered good practice. It provides several benefits, particularly in terms of performance when running queries that filter data or when calculations need to be performed on the data. By enforcing data types and data existence through the schema, the database can optimize certain operations, especially those that involve processing existing values. On the other hand, a disadvantage of using a schema is that it requires additional processing during insertions and modifications, as the database must validate that each new piece of data conforms to the defined constraints.\n",
    "\n",
    "The database schema we are about to define builds upon the previously documented logical model by incorporating the following elements:  \n",
    "- Defining attribute constraints: Each attribute will be associated with its corresponding data type.  \n",
    "- Primary key specification: For each entity in the logical model, the attributes that form the primary key will be explicitly defined.  \n",
    "- Mandatory attribute constraints: Attributes not included in the primary key will be marked as mandatory, ensuring data integrity. (Primary keys are inherently mandatory due to their constraint.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33ffa2bc-6bfb-4fba-a28d-954560379160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_terminals_schema execution time: 0.96s\n",
      "create_customers_schema execution time: 0.96s\n",
      "create_transaction_schema execution time: 1.34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_terminals_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT terminal_id_is_integer FOR (t:Terminal) REQUIRE t.terminal_id IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT terminal_id_key FOR (t:Terminal) REQUIRE t.terminal_id IS NODE KEY;\",\n",
    "        \"CREATE CONSTRAINT terminal_x_is_float FOR (t:Terminal) REQUIRE t.x_terminal_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT terminal_x_required FOR (t:Terminal) REQUIRE t.x_terminal_id IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT terminal_y_is_float FOR (t:Terminal) REQUIRE t.y_terminal_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT terminal_y_required FOR (t:Terminal) REQUIRE t.y_terminal_id IS NOT NULL;\"\n",
    "    ]\n",
    "    \n",
    "    return execute_query_commands(\"create_terminals_schema\", queries)\n",
    "\n",
    "def create_customers_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT customer_id_is_integer FOR (c:Customer) REQUIRE c.customer_id IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT customer_id_key FOR (c:Customer) REQUIRE c.customer_id IS NODE KEY;\",\n",
    "        \"CREATE CONSTRAINT customer_x_is_float FOR (c:Customer) REQUIRE c.x_customer_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_x_required FOR (c:Customer) REQUIRE c.x_customer_id IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_y_is_float FOR (c:Customer) REQUIRE c.y_customer_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_y_required FOR (c:Customer) REQUIRE c.y_customer_id IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_amount_is_float FOR (c:Customer) REQUIRE c.mean_amount IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_amount_required FOR (c:Customer) REQUIRE c.mean_amount IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_std_amount_is_float FOR (c:Customer) REQUIRE c.std_amount IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_std_amount_required FOR (c:Customer) REQUIRE c.std_amount IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_nb_tx_per_day_is_float FOR (c:Customer) REQUIRE c.mean_nb_tx_per_day IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_nb_tx_per_day_required FOR (c:Customer) REQUIRE c.mean_nb_tx_per_day IS NOT NULL;\"\n",
    "    ]\n",
    "    return execute_query_commands(\"create_customers_schema\", queries)\n",
    "\n",
    "def create_transaction_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT transaction_id_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.transaction_id IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT transaction_id_key FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.transaction_id IS RELATIONSHIP KEY;\",\n",
    "        \"CREATE CONSTRAINT tx_time_seconds_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_seconds IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_time_seconds_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_seconds IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_time_days_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_days IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_time_days_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_days IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_amount_is_float FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_amount IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT tx_amount_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_amount IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_day_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_day IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_day_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_day IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_date_month_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_month IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_date_month_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_month IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_year_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_year IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_date_year_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_year IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_time_is_localtime FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_time IS :: LOCAL TIME;\",\n",
    "        \"CREATE CONSTRAINT tx_date_time_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_time IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_is_boolean FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud IS :: BOOLEAN;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_is_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_scenario_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud_scenario IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_scenario_is_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud_scenario IS NOT NULL;\"\n",
    "    ]\n",
    "    return execute_query_commands(\"create_transaction_schema\", queries)\n",
    "\n",
    "create_terminals_schema()\n",
    "create_customers_schema()\n",
    "create_transaction_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8776b-b0f4-490f-a03e-747dbd8cc818",
   "metadata": {},
   "source": [
    "### 4.2) Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230cf7a-cfbc-4387-9f0d-e02f4f9cb736",
   "metadata": {},
   "source": [
    "In order to load data into Neo4j using CSV files, we must first consider where the Neo4j instance is. This is critical because the CSV files must be accessible from the machine running the Neo4j instance. There are two possible scenarios:\n",
    "- The CSV files reside on the machine running the Neo4j instance,\n",
    "- The CSV files are network resources that can be downloaded directly from a link.\n",
    "\n",
    "Since we are using a Neo4j instance managed by an external company, Aura, they obviously do not give us access to their servers, so we must choose the second option.\n",
    "\n",
    "This will have an impact on the performance of the data load, because the time indicated by the load procedure will include not only the time it takes to load the data from the file into the database, but also the time it takes the Neo4j instance to download the file. The download time is not negligible because, as we know, the network is much slower than a completely local approach. You can check this yourself by pasting the URL of the transaction CSV file into your browser and see how long it takes your machine to download the file.\n",
    "\n",
    "It's important to use a direct download link for the CSV files to make sure everything works. To share these files easily and quickly, I chose Dropbox because it offers a file sharing option with links that include a query parameter in the URL. This parameter, which appears as `&dl=1` at the end of the link, allows me to specify whether the link should be a direct download. This feature is critical for the Neo4j instance to download the file correctly. I also looked at other cloud storage systems, but the process of getting a direct download link was unnecessarily complex.\n",
    "\n",
    "Now let's look at the queries used to load the data into the database. Initially, I considered loading the data using the same example that the professor provided during the lessons: `USING PERIODIC COMMIT 1000 LOAD CSV FROM ...`, which is used to load data from a CSV file in batches of N rows per commit. However, since this directive is deprecated, I decided to use `LOAD CSV WITH HEADERS FROM ... CALL {...} IN TRANSACTIONS OF 1000 ROWS`, which gave me the same behavior.\n",
    "\n",
    "All three functions work similarly, with only the changes they make to the database changing. Each function downloads the CSV file specified by the link, then starts the batch job inside the `CALL{}` statement where the query creates the data instances in the database. At the end of the query in the `IN TRANSACTIONS OF 1000 ROWS` statement, we specify how many rows from the CSV to process before committing the changes to the database.\n",
    "\n",
    "In all 3 queries, the instances are created with a `MERGE` statement that sets the properties of the instance using the `ON CREATE SET` clause.\n",
    "- The `load_customers_with_available_terminals_from_csv()` function not only creates the customer, but also opens the list of terminals that the customer can operate on, matches them, and creates an `available` relationship between the customer and all matched terminals.\n",
    "  \n",
    "- The `load_transactions_from_csv()` function, before creating the transaction as described above, must match the customer and terminal to create the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d33bb27-1736-4e1a-aa16-b2e7d459dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_terminals_from_csv execution time: 1.80s\n",
      "load_customers_with_available_terminals_from_csv execution time: 2.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#E49A]  _: <CONNECTION> error: Failed to read from defunct connection IPv4Address(('45d4bc57.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.189.250.174', 7687))): OSError('No data')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_terminals_from_csv():\n",
    "    query = f\"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM \"{config[\"terminals_csv_link\"]}\" AS row FIELDTERMINATOR ';'\n",
    "        CALL {{\n",
    "            WITH row\n",
    "            CREATE (:Terminal {{terminal_id: toInteger(row.TERMINAL_ID),\n",
    "                                x_terminal_id: toFloat(row.x_terminal_id),\n",
    "                                y_terminal_id: toFloat(row.y_terminal_id)}})\n",
    "        }} IN TRANSACTIONS OF {config[\"lines_per_commit_call\"]} ROWS\n",
    "    \"\"\"\n",
    "    return execute_query_commands(\"load_terminals_from_csv\", [query])\n",
    "\n",
    "def load_customers_with_available_terminals_from_csv():    \n",
    "    query = f\"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM \"{config[\"customers_csv_link\"]}\" AS row FIELDTERMINATOR \";\" \n",
    "        CALL {{\n",
    "            WITH row\n",
    "            MERGE (c:Customer {{customer_id: toInteger(row.CUSTOMER_ID)}})\n",
    "            ON CREATE SET  \n",
    "                c.x_customer_id = toFloat(row.x_customer_id),\n",
    "                c.y_customer_id = toFloat(row.y_customer_id),\n",
    "                c.mean_amount = toFloat(row.mean_amount),\n",
    "                c.std_amount = toFloat(row.std_amount),\n",
    "                c.mean_nb_tx_per_day = toFloat(row.mean_nb_tx_per_day)\n",
    "            WITH c, row\n",
    "            WITH c, apoc.convert.fromJsonList(row.available_terminals) AS available_terminal_ids\n",
    "            UNWIND available_terminal_ids AS available_terminal_id\n",
    "            MATCH (t:Terminal {{terminal_id: available_terminal_id}})\n",
    "            MERGE (c)-[:Available]->(t)\n",
    "        }} IN TRANSACTIONS OF {config[\"lines_per_commit_call\"]} ROWS\n",
    "    \"\"\"\n",
    "\n",
    "    return execute_query_commands(\"load_customers_with_available_terminals_from_csv\", [query])\n",
    "\n",
    "def load_transactions_from_csv():\n",
    "    query = f\"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM \"{config[\"transactions_csv_link\"]}\" AS row FIELDTERMINATOR \";\" \n",
    "        CALL{{\n",
    "            WITH row\n",
    "\n",
    "            WITH row, \n",
    "                 split(row.TX_DATETIME, \" \") AS splitted_date_time\n",
    "            \n",
    "            WITH row,\n",
    "                 date(splitted_date_time[0]) AS parsed_date,\n",
    "                 localtime(splitted_date_time[1]) AS parsed_local_time\n",
    "\n",
    "            MATCH (c:Customer {{customer_id: toInteger(row.CUSTOMER_ID)}}), \n",
    "                (t:Terminal {{terminal_id: toInteger(row.TERMINAL_ID)}})\n",
    "            MERGE (c)-[transaction:Make_transaction {{transaction_id: toInteger(row.TRANSACTION_ID)}}]->(t)\n",
    "            ON CREATE SET \n",
    "                transaction.tx_time_seconds = toInteger(row.TX_TIME_SECONDS), \n",
    "                transaction.tx_time_days = toInteger(row.TX_TIME_DAYS),\n",
    "                transaction.tx_amount = toFloat(row.TX_AMOUNT), \n",
    "                transaction.tx_fraud = toBoolean(toInteger(row.TX_FRAUD)), \n",
    "                transaction.tx_fraud_scenario = toInteger(row.TX_FRAUD_SCENARIO),\n",
    "\n",
    "                transaction.tx_date_day = parsed_date.day,\n",
    "                transaction.tx_date_month = parsed_date.month,\n",
    "                transaction.tx_date_year = parsed_date.year, \n",
    "                transaction.tx_date_time = parsed_local_time \n",
    "        }} IN TRANSACTIONS OF {config[\"lines_per_commit_call\"]} ROWS\n",
    "    \"\"\"\n",
    "    return execute_query_commands(\"load_transactions_from_csv\", [query])\n",
    "\n",
    "\n",
    "load_terminals_from_csv()\n",
    "load_customers_with_available_terminals_from_csv()\n",
    "load_transactions_from_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffc76c-18c7-4ddd-a0b5-749390db5388",
   "metadata": {},
   "source": [
    "## 5) Workload\n",
    "In this section, I'll explain how I implemented the queries to efficiently respond to the various requirements outlined in the project specifications. Since the requested queries were not always precise in every detail, the analysis of each query will follow these key points:\n",
    "- Present the query as expressed in the project specifications;\n",
    "- Explain my interpretation of the requirement;\n",
    "- Explain how I built the query, providing the query code;\n",
    "- Look at the results;\n",
    "- Evaluate the performance of the query. Where necessary, to demonstrate the optimizations I have added, the execution plan will also be provided.\n",
    "\n",
    "Other query performance details are included in the dedicated section, where the execution times of different queries are compared across databases of different sizes.\n",
    "\n",
    "**Important:** Since I could not find a way to clear the caches in the free Neo4j instance (and I don't believe it is possible), when comparing the execution times of different versions of the same query, or the same query on different databases, it is crucial to ensure the accuracy of the timings by running them multiple times. Queries that change the state of the database, such as those that create schema, insert data, or modify existing data, should be run at most once per clean database instance. To run them again, it's necessary to restart the instance using the `clear_database()` function. This is because the schema-building functions are designed to fail if a schema rule already exists, ensuring that you are not using an unclean instance. The only exception to the rule for queries that change the state of the database and can be run as many times as needed is `create_composite_index_if_not_exists_on_Make_transaction_tx_date_month_and_tx_date_year()`. This query creates an index to optimize queries. If an index with the same name already exists, the function does nothing and does not create a new one. If the existing index does not match the one defined by the function, it is not critical for the database, but queries may not be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e41734-85e9-4fe2-be18-43cadd3afe18",
   "metadata": {},
   "source": [
    "### 5.1) Query A\n",
    "#### 5.1.1) Query A\n",
    "> For each customer checks that the spending frequency and the spending amounts of the last month is under the usual spending frequency and the spending amounts for the same period.\n",
    "\n",
    "- \"For each customer\": indicates that the query results must include all customers, even those for which it is not possible to calculate the requested data.  \n",
    "\n",
    "- \"last month\": refers to the month before the one specified as a parameter in the query. To call the Python function that executes this query, you must specify a partial date in \"yyyy-MM\" format as a parameter. This date is then used to calculate the `first_of_previous_month variable` within the query. This variable represents the first day of the month immediately preceding the given date. When determining the value of `first_of_previous_month`, only the month and year are taken into account, ensuring that the query correctly filters data relevant to the previous month.  \n",
    "\n",
    "- \"Usual spending frequency and spending amounts for the same period\": I interpreted this to mean that the spending frequency and amount must be calculated as the average of all spending frequencies and amounts recorded in the database that match the same month but correspond to a year earlier than the `first_of_previous_month` variable.\n",
    "\n",
    "#### 5.1.2) A1 query code\n",
    "Let's provide a first version of the A query.\n",
    "\n",
    "The query starts by calculating the date corresponding to the first day of the previous month relative to the date provided to the Python function. This date is stored in the `first_of_previous_month` variable.\n",
    "\n",
    "Next, all customers are matched to ensure that none are excluded from the final result of the query. This is done because the following `WHERE` clauses do not filter out customers, and all subsequent matches are `OPTIONAL MATCH`.\n",
    "\n",
    "The first `OPTIONAL MATCH` is used to retrieve the transaction history for the same period, these transactions are stored in the variable `tx_prev_month_all_prev_year`.\n",
    "\n",
    "The following `WITH` clause is special because instead of counting the `tx_prev_month_all_prev_year` and summing their amounts, it returns `NULL` for both values if no transactions are found in the history. This is useful for distinguishing, in the final result, customers for whom no significant transaction history is found (and therefore no calculations can be performed) from those for whom a history is available (and calculations can be performed as required by the query).\n",
    "\n",
    "The next `WITH` clause calculates the averages of the results just calculated, `tx_prev_month_prev_year_total_amount` and `tx_prev_month_prev_year_montly_freq`, yielding `tx_prev_month_all_prev_year_total_amount_avg` and `tx_prev_month_all_prev_year_montly_freq_avg`. The `AVG` operator preserves the `NULL` value when calculating based on `NULL`, so if there are no transactions, `AVG(NULL)` will return `NULL`.\n",
    "\n",
    "The last `OPTIONAL MATCH` performs the same calculations as the previous one, but now on transactions `tx` that have the same month and year as `first_of_previous_month`. Unlike before, there is no need to distinguish between customers with and without transactions at this stage, as this distinction is made in the `RETURN` clause by referencing the historical data.\n",
    "\n",
    "The last `WITH` calculates `total_amount_prev_month` and `monthly_freq_prev_month` which represent the total transaction amount and transaction frequency of all `tx`. These two values are then used in the `RETURN` stage to determine if they are below the usual average transaction amount and frequency.\n",
    "\n",
    "In the `RETURN` statement, if the customer has historical data for the same period (indicated by `tx_prev_month_all_prev_year_monthly_freq_avg IS NOT NULL`), then we check whether `total_amount_prev_month < tx_prev_month_all_prev_year_total_amount_avg` and `monthly_freq_prev_month < tx_prev_month_all_prev_year_monthly_freq_avg`. It is important to note that in this scenario the customer may not have any `tx`. However, since historical data is available, the absence of `tx` does not indicate missing data in the database. Instead, it means that the customer has not made any transactions in the same month and year as `first_of_previous_month`.\n",
    "\n",
    "If a customer doesn't have the same period of historical data, we can't give a meaningful answer, so we respond with a `NULL` value in both the `is_under_total_amount_avg_of_same_period` and `is_under_monthly_freq_avg_of_same_period` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7b78d7-88dd-47ef-8320-94f6655c6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to retrieve routing information\n",
      "Unable to retrieve routing information\n",
      "Unable to retrieve routing information\n",
      "Unable to retrieve routing information\n",
      "Unable to retrieve routing information\n",
      "Unable to retrieve routing information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_a1 execution time: 58.79s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>is_under_total_amount_avg_of_same_period</th>\n",
       "      <th>is_under_monthly_freq_avg_of_same_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     c  \\\n",
       "0    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "1    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "2    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "3    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "4    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "..                                                 ...   \n",
       "195  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "196  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "197  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "198  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "199  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "\n",
       "    is_under_total_amount_avg_of_same_period  \\\n",
       "0                                       None   \n",
       "1                                       None   \n",
       "2                                       None   \n",
       "3                                       None   \n",
       "4                                       None   \n",
       "..                                       ...   \n",
       "195                                     None   \n",
       "196                                     None   \n",
       "197                                     None   \n",
       "198                                     None   \n",
       "199                                     None   \n",
       "\n",
       "    is_under_monthly_freq_avg_of_same_period  \n",
       "0                                       None  \n",
       "1                                       None  \n",
       "2                                       None  \n",
       "3                                       None  \n",
       "4                                       None  \n",
       "..                                       ...  \n",
       "195                                     None  \n",
       "196                                     None  \n",
       "197                                     None  \n",
       "198                                     None  \n",
       "199                                     None  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year_and_month_under_analesis is a string that contains a year and a month in the format yyyy-MM\n",
    "def query_a1(year_and_month_under_analesis):\n",
    "    query = f\"\"\"\n",
    "            WITH date.truncate('month', date(\"{year_and_month_under_analesis}\" + \"-01\") ) - duration({{months: 1}}) AS first_of_previous_month\n",
    "            \n",
    "            MATCH (c:Customer)\n",
    "\n",
    "            OPTIONAL MATCH (c)-[tx_prev_month_all_prev_year:Make_transaction]->(:Terminal)\n",
    "            WHERE \n",
    "                tx_prev_month_all_prev_year.tx_date_month = first_of_previous_month.month\n",
    "                AND tx_prev_month_all_prev_year.tx_date_year < first_of_previous_month.year\n",
    "            WITH\n",
    "                first_of_previous_month,\n",
    "                c,\n",
    "                tx_prev_month_all_prev_year.tx_date_year as year, \n",
    "                CASE \n",
    "                    WHEN COUNT(tx_prev_month_all_prev_year)>0 THEN SUM(tx_prev_month_all_prev_year.tx_amount)\n",
    "                    ELSE NULL\n",
    "                END AS tx_prev_month_prev_year_total_amount, \n",
    "\n",
    "                CASE \n",
    "                    WHEN  COUNT(tx_prev_month_all_prev_year)>0 THEN COUNT(tx_prev_month_all_prev_year)\n",
    "                    ELSE NULL\n",
    "                END AS tx_prev_month_prev_year_montly_freq\n",
    "            WITH\n",
    "            first_of_previous_month,\n",
    "            c, \n",
    "            AVG(tx_prev_month_prev_year_total_amount) AS tx_prev_month_all_prev_year_total_amount_avg, \n",
    "            AVG(tx_prev_month_prev_year_montly_freq) AS tx_prev_month_all_prev_year_montly_freq_avg\n",
    "\n",
    "            OPTIONAL MATCH (c)-[tx:Make_transaction]->(:Terminal)\n",
    "            WHERE \n",
    "                tx.tx_date_month = first_of_previous_month.month AND \n",
    "                tx.tx_date_year = first_of_previous_month.year\n",
    "            WITH\n",
    "                c,\n",
    "                SUM(tx.tx_amount) AS total_amount_prev_month, \n",
    "                COUNT(tx) AS monthly_freq_prev_month,\n",
    "                tx_prev_month_all_prev_year_total_amount_avg,\n",
    "                tx_prev_month_all_prev_year_montly_freq_avg\n",
    "\n",
    "            RETURN\n",
    "                c,\n",
    "\n",
    "                CASE \n",
    "                    WHEN tx_prev_month_all_prev_year_total_amount_avg IS NULL THEN NULL\n",
    "                    ELSE total_amount_prev_month < tx_prev_month_all_prev_year_total_amount_avg\n",
    "                END AS is_under_total_amount_avg_of_same_period,\n",
    "\n",
    "                CASE \n",
    "                    WHEN tx_prev_month_all_prev_year_montly_freq_avg IS NULL THEN NULL\n",
    "                    ELSE monthly_freq_prev_month < tx_prev_month_all_prev_year_montly_freq_avg\n",
    "                END AS is_under_monthly_freq_avg_of_same_period\n",
    "    \"\"\"\n",
    "\n",
    "    return execute_query_df(\"query_a1\",query)\n",
    "\n",
    "month_and_year_under_analesis = \"2023-05\"\n",
    "query_a1(month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63752-1b6a-45d7-a369-d4105a3b2af5",
   "metadata": {},
   "source": [
    "#### 5.1.3) A1 Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c1145-46af-42c6-bc35-746eaf941ddc",
   "metadata": {},
   "source": [
    "In order to improve the performance of the query, since it matches the data on `make_transaction.tx_date_month` and `make_transaction.tx_date_year`, we can create a compound index on these two fields.\n",
    "After that, we can call the query again, passing the same argument, and look at the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5beb648-9448-4bcd-a68e-3691b213d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_composite_index_if_not_exists_on_Make_transaction_tx_date_month_and_tx_date_year execution time: 0.48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_composite_index_if_not_exists_on_Make_transaction_tx_date_month_and_tx_date_year():\n",
    "    query = \"CREATE INDEX composite_index_on_tx_date_year_and_month IF NOT EXISTS FOR ()-[tx:Make_transaction]-() ON (tx.tx_date_month, tx.tx_date_year)\"\n",
    "    return execute_query_commands(\"create_composite_index_if_not_exists_on_Make_transaction_tx_date_month_and_tx_date_year\", [query])\n",
    "\n",
    "create_composite_index_if_not_exists_on_Make_transaction_tx_date_month_and_tx_date_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e77c988-6f70-4ad0-a490-1d7cc91ae759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_a1 execution time: 1.03s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>is_under_total_amount_avg_of_same_period</th>\n",
       "      <th>is_under_monthly_freq_avg_of_same_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     c  \\\n",
       "0    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "1    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "2    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "3    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "4    (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "..                                                 ...   \n",
       "195  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "196  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "197  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "198  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "199  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "\n",
       "    is_under_total_amount_avg_of_same_period  \\\n",
       "0                                       None   \n",
       "1                                       None   \n",
       "2                                       None   \n",
       "3                                       None   \n",
       "4                                       None   \n",
       "..                                       ...   \n",
       "195                                     None   \n",
       "196                                     None   \n",
       "197                                     None   \n",
       "198                                     None   \n",
       "199                                     None   \n",
       "\n",
       "    is_under_monthly_freq_avg_of_same_period  \n",
       "0                                       None  \n",
       "1                                       None  \n",
       "2                                       None  \n",
       "3                                       None  \n",
       "4                                       None  \n",
       "..                                       ...  \n",
       "195                                     None  \n",
       "196                                     None  \n",
       "197                                     None  \n",
       "198                                     None  \n",
       "199                                     None  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_a1(month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9086f3-1741-45ca-9f8d-3b9b35428042",
   "metadata": {},
   "source": [
    "As you can see in the execution plan image below, the query does not use the index at all. This is because in the initial `MATCH` clause, we do not directly filter the transactions. Instead, we first match the customers, which prevents the query from using the index efficiently.  \n",
    "\n",
    "In fact, the only index used is on the customers, and it is only used to retrieve all the customer nodes without doing any filtering. As for the transactions, no index is used either in the initial filtering or in the subsequent `OPTIONAL MATCH`, which further contributes to the inefficiency of the query.  \n",
    "\n",
    "To generate the execution plan shown in the image, you simply need to prefix the query with the word `EXPLAIN` in Neo4j.  \n",
    "\n",
    "<img src=\"./assets/Execution plan query A1.svg\" style=\"width:700px;\">.\n",
    "\n",
    "#### 5.1.4) A2 Query Code\n",
    "By slightly modifying the query to omit the \"for all customers\" clause and only display customers with historical data, we can significantly improve performance by leveraging the index. This tweak involves removing the first `MATCH` clause and changing the second `OPTIONAL MATCH` to a regular `MATCH`.  \n",
    "\n",
    "This change means that the results will no longer include customers with `NULL` values in the columns `tx_prev_month_all_prev_year_total_amount_avg` and `tx_prev_month_all_prev_year_montly_freq_avg`, as these customers are directly excluded by the first `MATCH` clause.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e1ba842-727c-41a5-9d17-4e09cd7f9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_a2 execution time: 0.73s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>is_under_total_amount_avg_of_same_period</th>\n",
       "      <th>is_under_monthly_freq_avg_of_same_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [c, is_under_total_amount_avg_of_same_period, is_under_monthly_freq_avg_of_same_period]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year_and_month_under_analesis is a string that contains a year and a month in the format yyyy-MM\n",
    "def query_a2(year_and_month_under_analesis):\n",
    "    query = f\"\"\"\n",
    "            WITH date.truncate('month', date(\"{year_and_month_under_analesis}\" + \"-01\") ) - duration({{months: 1}}) AS first_of_previous_month\n",
    "\n",
    "            MATCH (c)-[tx_prev_month_all_prev_year:Make_transaction]->(:Terminal)\n",
    "            WHERE \n",
    "                tx_prev_month_all_prev_year.tx_date_month = first_of_previous_month.month\n",
    "                AND tx_prev_month_all_prev_year.tx_date_year < first_of_previous_month.year\n",
    "            WITH\n",
    "                first_of_previous_month,\n",
    "                c,\n",
    "                tx_prev_month_all_prev_year.tx_date_year as year,\n",
    "                SUM(tx_prev_month_all_prev_year.tx_amount)  AS tx_prev_month_prev_year_total_amount, \n",
    "                COUNT(tx_prev_month_all_prev_year) AS tx_prev_month_prev_year_montly_freq\n",
    "            WITH\n",
    "            first_of_previous_month,\n",
    "            c, \n",
    "            AVG(tx_prev_month_prev_year_total_amount) AS tx_prev_month_all_prev_year_total_amount_avg, \n",
    "            AVG(tx_prev_month_prev_year_montly_freq) AS tx_prev_month_all_prev_year_montly_freq_avg\n",
    "\n",
    "            OPTIONAL MATCH (c)-[tx:Make_transaction]->(:Terminal)\n",
    "            WHERE \n",
    "                tx.tx_date_month = first_of_previous_month.month AND \n",
    "                tx.tx_date_year = first_of_previous_month.year\n",
    "            WITH\n",
    "                c,\n",
    "                SUM(tx.tx_amount) AS total_amount_prev_month, \n",
    "                COUNT(tx) AS monthly_freq_prev_month,\n",
    "                tx_prev_month_all_prev_year_total_amount_avg,\n",
    "                tx_prev_month_all_prev_year_montly_freq_avg\n",
    "\n",
    "            RETURN\n",
    "                c, \n",
    "                total_amount_prev_month < tx_prev_month_all_prev_year_total_amount_avg  AS is_under_total_amount_avg_of_same_period,\n",
    "                monthly_freq_prev_month < tx_prev_month_all_prev_year_montly_freq_avg AS is_under_monthly_freq_avg_of_same_period\n",
    "            \"\"\"\n",
    "    \n",
    "    return execute_query_df(\"query_a2\",query)\n",
    "query_a2(month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476082d2-7dc3-46ba-a145-4ddb99a62de7",
   "metadata": {},
   "source": [
    "#### 5.1.5) A2 Performances\n",
    "As shown in the execution plan image below, the query now uses the index we created specifically for filtering transactions. Unlike the initial version, which did not use an index on the transactions, this optimized approach ensures that the query uses the index effectively to improve performance during the filtering process.\n",
    "\n",
    "<img src=\"./assets/Execution plan query A2.svg\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad3fc2-d9ea-4d44-a734-6ab076d36be7",
   "metadata": {},
   "source": [
    "### 5.2) Query B\n",
    "#### 5.2.1) Query Request\n",
    "> For each terminal identify the possible fraudulent transactions. The fraudulent transactions are those whose import is higher than 20% of the maximal import of the transactions executed on the same terminal in the last month.\n",
    "\n",
    "- \"For each terminal\": This means that the query results must include all terminals, even those for which it is not possible to identify fraudulent transactions.\n",
    "\n",
    "- \"Last month\": refers to data from the month prior to the month specified as a parameter. Similar to the previous query, this query is parameterized by passing a partial date in \"yyyy-MM\" format to Python. This date is used to calculate the `first_of_previous_month` variable, which represents the first day of the month prior to the given date. In addition, the query includes a reference to the first day of the current month, stored in the `today` variable, for further calculations or filtering as needed. \n",
    "\n",
    "#### 5.2.2) B1 query code\n",
    "The query starts by storing the given date in the `today` variable and calculating the first day of the previous month stored in `first_of_previous_month`. \n",
    "\n",
    "Next, all terminals are matched to ensure that none are excluded from the final result of the query. This is done because the following `WHERE` clauses do not filter out any terminals, and all subsequent matches are `OPTIONAL MATCH`.\n",
    "\n",
    "The first `OPTIONAL MATCH` retrieves transactions made on terminals during the month and year corresponding to `first_of_previous_month`. These transactions are stored in the variable `tx_prev_month`. However, some terminals may not have any transactions for the specified period, in which case `tx_prev_month` will be empty for those terminals.\n",
    "\n",
    "The query then calculates the fraud detection threshold using a WITH statement. The fraud amount limit, stored in the variable `tx_amount_fraud_limit`, is defined as 20% above the maximum transaction amount from the previous month. For terminals where no transactions were found in `tx_prev_month`, the fraud amount limit remains `NULL`.\n",
    "\n",
    "The next step uses another `OPTIONAL MATCH` to retrieve transactions for the current month, filtering by the same month and year as `today`. These transactions are stored in the variable `tx_current_month`. Using the calculated fraud amount limit, the query identifies fraudulent transactions by collecting those in `tx_current_month` where the transaction amount exceeds `tx_amount_fraud_limit`. This collection is stored in `fraud_txs_current_month`. If `tx_amount_fraud_limit` is `NULL`, the condition will always evaluate false, resulting in an empty collection for the terminal.\n",
    "\n",
    "Finally, the `RETURN` statement distinguishes between two problematic cases when a terminal has an empty `fraud_txs_current_month` collection. In the first case, the fraud amount limit could not be calculated, making it impossible to determine whether the terminal had fraudulent transactions. In the second case, the limit was calculated but no fraudulent transactions were identified for that terminal in the current month. To resolve this ambiguity, the query replaces empty collections in `fraud_txs_current_month` with the value `NULL` whenever `tx_amount_fraud_limit IS NULL`. This approach ensures clarity in the results by distinguishing between the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31f3bfcc-2c64-4640-b515-3d3d9346c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_b1 execution time: 0.80s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>fraud_txs_current_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(y_terminal_id, terminal_id, x_terminal_id)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              t fraud_txs_current_month\n",
       "0   (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "1   (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "2   (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "3   (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "4   (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "..                                          ...                     ...\n",
       "45  (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "46  (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "47  (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "48  (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "49  (y_terminal_id, terminal_id, x_terminal_id)                    None\n",
       "\n",
       "[50 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year_and_month_under_analesis is a string that contains a year and a month in the format yyyy-MM\n",
    "def query_b1(year_and_month_under_analesis):\n",
    "    query = f\"\"\"\n",
    "            WITH date(\"{year_and_month_under_analesis}\" + \"-01\") AS today\n",
    "            WITH today, date.truncate('month', today ) - duration({{months: 1}}) AS first_of_previous_month\n",
    "\n",
    "            MATCH (t:Terminal)\n",
    "\n",
    "            OPTIONAL MATCH (:Customer)-[tx_prev_month:Make_transaction]->(t)\n",
    "            WHERE \n",
    "                tx_prev_month.tx_date_month = first_of_previous_month.month\n",
    "                AND tx_prev_month.tx_date_year = first_of_previous_month.year\n",
    "\n",
    "            with today, t, max(tx_prev_month.tx_amount) * 1.2 as tx_amount_fraud_limit\n",
    "\n",
    "            OPTIONAL MATCH (:Customer)-[tx_current_month:Make_transaction]->(t)\n",
    "            WHERE \n",
    "                tx_current_month.tx_date_month = today.month\n",
    "                AND tx_current_month.tx_date_year = today.year\n",
    "\n",
    "            WITH \n",
    "                t, \n",
    "                tx_amount_fraud_limit,\n",
    "                COLLECT(CASE \n",
    "                    WHEN tx_current_month.tx_amount > tx_amount_fraud_limit THEN tx_current_month \n",
    "                    ELSE NULL \n",
    "                END) AS fraud_txs_current_month\n",
    "\n",
    "            RETURN \n",
    "                t, \n",
    "                CASE \n",
    "                    WHEN tx_amount_fraud_limit IS NULL THEN NULL\n",
    "                    ELSE fraud_txs_current_month\n",
    "                END AS fraud_txs_current_month\n",
    "            \"\"\"\n",
    "\n",
    "    return execute_query_df(\"query_b1\",query)\n",
    "query_b1(month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e54644-ad05-4920-a6a5-8816e6aa4e1c",
   "metadata": {},
   "source": [
    "#### 5.2.3) B1 Performance\n",
    "To improve the performance of the query, since it matches the data on `make_transaction.tx_date_month` and `make_transaction.tx_date_year`, we can reuse the composite index previously created with the Python function `create_composite_index_if_not_exists_on_make_transaction_tx_date_month_and_tx_date_year()`.\n",
    "\n",
    "As we can see in the execution plan of the query shown below, the same behavior observed in the previous query occurs here as well. In particular, the first `MATCH` clause, which matches all terminals, prevents the index from being used to filter the transactions.  \n",
    "\n",
    "In fact, the only index used is on the terminals, and it is only used to retrieve all the terminal nodes without performing any filtering. As for the transactions, no index is used either in the initial filtering or in the subsequent `OPTIONAL MATCH`, which further contributes to the inefficiency of the query.  \n",
    "\n",
    "<img src=\"./assets/Execution plan query B1.svg\" style=\"width:1000px;\">.\n",
    "\n",
    "#### 5.2.4) B2 Query Code\n",
    "By slightly modifying the query to omit the \"for all terminals\" clause and display only terminals with `tx_amount_fraud_limit`, we can improve performance by using the index. This tweak involves removing the first `MATCH` clause and changing the second `OPTIONAL MATCH` to a regular `MATCH`.  \n",
    "\n",
    "This change means that the results will no longer include terminals with `NULL` values in the `fraud_txs_current_month` column, as these terminals are directly excluded by the first `MATCH` clause.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "849393fa-5315-41f2-a1d3-c30684ab45ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_b2 execution time: 0.72s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>fraud_txs_current_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [t, fraud_txs_current_month]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year_and_month_under_analesis is a string that contains a year and a month in the format yyyy-MM\n",
    "def query_b2(year_and_month_under_analesis):\n",
    "    query = f\"\"\"\n",
    "            WITH date(\"{year_and_month_under_analesis}\" + \"-01\") AS today\n",
    "            WITH today, date.truncate('month', today ) - duration({{months: 1}}) AS first_of_previous_month\n",
    "\n",
    "            MATCH (:Customer)-[tx_prev_month:Make_transaction]->(t:Terminal)\n",
    "            WHERE \n",
    "                tx_prev_month.tx_date_month = first_of_previous_month.month\n",
    "                AND tx_prev_month.tx_date_year = first_of_previous_month.year\n",
    "\n",
    "            with today, t, max(tx_prev_month.tx_amount) * 1.2 as tx_amount_fraud_limit\n",
    "\n",
    "            OPTIONAL MATCH (:Customer)-[tx_current_month:Make_transaction]->(t)\n",
    "            WHERE \n",
    "                tx_current_month.tx_date_month = today.month\n",
    "                AND tx_current_month.tx_date_year = today.year\n",
    "\n",
    "            RETURN \n",
    "                t,\n",
    "                COLLECT( \n",
    "                    CASE \n",
    "                        WHEN tx_current_month.tx_amount > tx_amount_fraud_limit THEN tx_current_month \n",
    "                        ELSE NULL \n",
    "                    END \n",
    "                )AS fraud_txs_current_month\n",
    "            \"\"\"\n",
    "   \n",
    "    return execute_query_df(\"query_b2\",query)\n",
    "query_b2(month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228735b7-22b4-47c0-9997-f4cc0ce70bcb",
   "metadata": {},
   "source": [
    "#### 5.2.4) B2 Execution\n",
    "As shown in the execution plan image below, the query now uses the index we created specifically for filtering transactions. Unlike the initial version, where no index was used on the transactions, this optimized approach ensures that the query uses the index effectively to improve performance during the filtering process.\n",
    "\n",
    "<img src=\"./assets/Execution plan query B2.svg\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f7927-bb15-46bd-a42c-1bd31bd9152b",
   "metadata": {},
   "source": [
    "### 5.3) Query C\n",
    "#### 5.3.1) Query request\n",
    "> Given a user u, determine the “co-customer-relationships CC of degree k”. A user u’ is a co-customer of u if you can determine a chain “u1-t1-u2-t2-…tk-1-uk“ such that u1=u, uk=u’, and for each 1<=I,j<=k, ui <> uj, and t1,..tk-1 are the terminals on which a transaction has been\n",
    "executed. Therefore, CCk(u)={u’| a chain exists between u and u’ of degree k}. Please, note that depending on the adopted model, the computation of CCk(u) could be quite complicated. Consider therefore at least the computation of CC3(u) (i.e. the co-costumer relationships of degree 3).\n",
    "\n",
    "This request is very precise and needs no further elaboration. What I would like to emphasize is the proposed solution, which uses an APOC function for efficient graph traversal. This approach will prove to be highly efficient, allowing us to surpass the co-client of degree `k` in remarkably short processing times.\n",
    "\n",
    "#### 5.3.2) C query code\n",
    "The Python function that executes the query takes two parameters: `customer_id`, representing the starting customer, and `k`, representing the degree of the co-customer. The query uses APOC's `expandConfig` function to efficiently explore relationships up to a specified level. Starting from the customer node with the same ID as the passed `customer_id`, it navigates through `make_transaction` relationships to `terminal` or other `customer` nodes. The `relationshipFilter` and `labelFilter` parameters allow the query to specify the types of relationships and node labels to be considered. The `maxLevel` parameter limits the exploration depth, ensuring that only paths with length <= `k` are returned. The `uniqueness: 'NODE_GLOBAL'` setting guarantees that each node in the path appears only once.\n",
    "\n",
    "To focus only on paths of exact length `k`, a `WHERE` clause filters the results after the `WITH` clause. Finally, the `RETURN` statement selects only the last node in each qualified path that represents the desired co-customer of interest.\n",
    "\n",
    "The `k` passed to the Python function is reworked in the query because the `maxLevel` parameter must specify the maximum number of nodes in the path. Since each co-customer needs a terminal between itself and the immediately lower-level co-customer, the Python `k` becomes `(k - 1) * 2` in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6757b81-4740-4ae1-a397-4b803289a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_c execution time: 0.60s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO_Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [CO_Customer]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#customer_id is an integer that indicates the customer_id property of :Customer\n",
    "#k is an integer that indicates the different customers involved in the chain described in the project track\n",
    "def query_c(customer_id, k):\n",
    "    query = f\"\"\"\n",
    "            WITH {k-1} * 2 AS k\n",
    "            MATCH (start:Customer {{customer_id: {customer_id}}})\n",
    "            CALL apoc.path.expandConfig(start, {{\n",
    "                relationshipFilter: 'Make_transaction',\n",
    "                labelFilter: 'Terminal|Customer',\n",
    "                maxLevel: k,\n",
    "                uniqueness: 'NODE_GLOBAL'\n",
    "            }}) YIELD path\n",
    "\n",
    "            WITH path\n",
    "            WHERE length(path) = k\n",
    "            RETURN nodes(path)[-1].customer_id AS CO_Customer\n",
    "            \"\"\"\n",
    "    return execute_query_df(\"query_c\",query)\n",
    "query_c(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c835d-084b-4aa0-81dc-42df02430941",
   "metadata": {},
   "source": [
    "#### 5.3.3) C Performance\n",
    "I was pleasantly surprised by the performance of this solution, especially considering that the query's requirements represent a potentially exponential task. Before arriving at this query, I tried several approaches with very poor results. Even calculating \\(CC_3(...)\\) (the co-customer of degree \\(k = 3\\) starting from the customer with `customer_id = ...`) took an enormous amount of time, and attempting `k > 3` resulted in no response, likely due to the excessive computation time required. \n",
    "\n",
    "The query is also highly efficient because by using the `uniqueness: 'NODE_GLOBAL'` many paths are discarded, significantly reducing the number of possible paths. This happens because, despite having a large number of `make_transactions` relationships, the customers and terminals have fewer relationships to the transactions. Since the requirement is that customers and terminals must be unique within the path, many paths are filtered out, further reducing the computational load.\n",
    "\n",
    "With the proposed solution, however, it is possible to go well beyond `k = 3` while still maintaining remarkably low execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc82b711-d6f6-4456-b33a-6e159ad4dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_c execution time: 0.47s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO_Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [CO_Customer]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_c(5, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc6e58-dd8b-454a-8818-0e6f05d6c930",
   "metadata": {},
   "source": [
    "To visualise the chains of customers and terminals, I ran the query in the Neo4j console, which returned all the paths starting from `customer_id = 5` and reaching the customers returned by *query_c(5, 8)*.\n",
    "\n",
    "The data displayed inside the nodes in the image is not particularly meaningful, as it shows one of the properties of the nodes, which in this case is not relevant to the visualization.\n",
    "\n",
    "<img src=\"./assets/Query C path.png\" style=\"width:1300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac8d95-d56f-440a-9ce7-331dd47b295a",
   "metadata": {},
   "source": [
    "### 5.4) Query D\n",
    "#### 5.4.1) Query request\n",
    "> i. Each transaction should be extended with:\n",
    "\n",
    "> > 1. The period of the day {morning, afternoon, evening, night} in which the transaction has been executed.\n",
    "  \n",
    "> > 2. The kind of products that have been bought through the transaction {hightech, food, clothing, consumable, other}\n",
    "\n",
    "> > 3. The feeling of security expressed by the user. This is an integer value between 1 and 5 expressed by the user when conclude the transaction.\n",
    "\n",
    "> The values can be chosen randomly.\n",
    "\n",
    "> ii. Customers that make more than three transactions from the same terminal expressing a similar average feeling of security should be connected as\n",
    "“buying_friends”. Therefore also this kind of relationship should be explicitly stored in the NOSQL database and can be queried. Note, two average feelings of security are considered similar when their difference is lower than 1.\n",
    "\n",
    "The query is clearly worded and leaves no room for alternative interpretations, so there is no need to explain it further. For simplicity, we will split this query into two separate queries: `query_di`, which performs point i, and `query_dii`, which performs point ii.\n",
    "\n",
    "The approach for both queries is similar, as both use APOC's `iterate` function, which allows batch tasks to be defined and executed in parallel, similar to the `CALL{}` used earlier in Section 4. The `iterate` function takes three parameters: the query to be run, the size of the batch, and whether the task should be run in parallel, and proceeds to do the work.\n",
    "\n",
    "#### 5.4.2) Di query code\n",
    "The `query_di` itself has been split into two queries, each with its own Python function: \n",
    "1. the first query is the core one that uses the `iterate` function to modify the data, it retrieves all the transactions with the `MATCH` function and adds the 3 requested properties, selecting them randomly with the `CASE` function and using `rand()` to calculate the condition;\n",
    "\n",
    "2. the second query adds the constraints for the new properties to the transactions schema. Unlike the data loading process, the schema creation is done after the data modification. This is because the data already exists and creating the schema for the new data before adding them the costraint creation would not work because the existing data wouldn't satisfy the new constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1485b155-05ec-429e-b5af-126fd465ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_di execution time: 0.49s\n",
      "create_transaction_extended_schema execution time: 0.76s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_di():\n",
    "    query = f\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "            'MATCH (c:Customer)-[transaction:Make_transaction]->(t:Terminal) \n",
    "            RETURN transaction',\n",
    "            'SET transaction.tx_day_period = CASE toInteger(rand() * 4)\n",
    "                                                WHEN 0 THEN \"morning\" \n",
    "                                                WHEN 1 THEN \"afternoon\" \n",
    "                                                WHEN 2 THEN \"evening\" \n",
    "                                                ELSE \"night\" \n",
    "                                            END,\n",
    "                transaction.tx_products_type = CASE toInteger(rand() * 5) \n",
    "                                                    WHEN 0 THEN \"high-tech\" \n",
    "                                                    WHEN 1 THEN \"food\" \n",
    "                                                    WHEN 2 THEN \"clothing\" \n",
    "                                                    WHEN 3 THEN \"consumable\" \n",
    "                                                    ELSE \"other\" \n",
    "                                                END,\n",
    "                transaction.tx_security_feeling = toInteger(rand() * 5) + 1',\n",
    "            {{batchSize: {config[\"lines_per_commit_apoc\"]}, parallel: {config[\"parallel_loading\"]}}}\n",
    "        )\n",
    "    \"\"\"\n",
    "    return execute_query_commands(\"query_di\", [query])\n",
    "\n",
    "def create_transaction_extended_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT tx_day_period_is_string FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_day_period IS :: STRING;\",\n",
    "        \"CREATE CONSTRAINT tx_day_period_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_day_period IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_products_type_is_string FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_products_type IS :: STRING;\",\n",
    "        \"CREATE CONSTRAINT tx_products_type_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_products_type IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_security_feeling_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_security_feeling IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_security_feeling_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_security_feeling IS NOT NULL;\",\n",
    "    ]\n",
    "    return execute_query_commands(\"create_transaction_extended_schema\", queries)\n",
    "    \n",
    "query_di()\n",
    "create_transaction_extended_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971be973-59ed-44ee-aca1-50c43991bb85",
   "metadata": {},
   "source": [
    "#### 5.4.3) Dii Query Code  \n",
    "The query begins with the first `MATCH`, identifying all customers `c1` who have made at least three transactions at a terminal `t` and calculates the average of the `tx_security_feeling` property for these transactions, storing the result in `avg_tx1_security_feeling`. It then searches for other customers `c2` who have also made at least three transactions at the same terminal, calculating their average `tx_security_feeling` and storing it in `avg_tx2_security_feeling`.\n",
    "\n",
    "Once the pairs of customers `c1` and `c2` sharing the same terminal with at least 3 transactions are identified, the query checks whether the absolute difference between their average security feelings values are less than 1. This condition ensures that the two customers have similar transaction security experiences at the same terminal. If the condition is met, the query creates a `buying_friends` relationship between the two customers.  \n",
    "\n",
    "Since `buying_friends` is a symmetric relationship, the condition `c1 < c2` is used to ensure that the relationship is created only once for each pair. This prevents duplicate relationships from being formed (e.g., both `c1 -> c2` and `c2 -> c1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cc9d89a-fa23-49a5-bd71-f8eaede359ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_dii execution time: 0.46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_dii():\n",
    "    query = f\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "            '\n",
    "                MATCH (c1:Customer)-[tx1:Make_transaction]->(t:Terminal) \n",
    "                WITH c1, t, COUNT(tx1) AS count_tx1, avg(tx1.tx_security_feeling) as avg_tx1_security_feeling\n",
    "                WHERE count_tx1 > 3\n",
    "\n",
    "                MATCH (c2:Customer)-[tx2:Make_transaction]->(t:Terminal) \n",
    "                WITH c1, c2, t, avg_tx1_security_feeling, COUNT(tx2) AS count_tx2, avg(tx2.tx_security_feeling) as avg_tx2_security_feeling\n",
    "                WHERE \n",
    "                    count_tx2 > 3 AND \n",
    "                    c1 < c2 AND \n",
    "                    (abs(avg_tx1_security_feeling - avg_tx2_security_feeling) < 1)\n",
    "\n",
    "                RETURN c1, c2\n",
    "            ',\n",
    "            '\n",
    "                MERGE (c1)-[:buying_friends]-(c2)\n",
    "            ',\n",
    "            {{batchSize: {config[\"lines_per_commit_apoc\"]}, parallel: {config[\"parallel_loading\"]}}}\n",
    "        )\n",
    "    \"\"\"\n",
    "    return execute_query_commands(\"query_dii\",[query])\n",
    "    \n",
    "query_dii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5ab61-875c-4e5c-910d-eeee7256ced0",
   "metadata": {},
   "source": [
    "#### 5.4.4) Di and Dii Performances\n",
    "For both queries the performance is excellent and I have not produced optimised versions, the execution plan is not shown below as it is unnecessary as all the work is done in a single block `APOC.iterate` which ensures parallelised batch work giving us efficient queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c045740-2ea2-45ff-b195-e1963268140a",
   "metadata": {},
   "source": [
    "### 5.5) Query E\n",
    "\n",
    "#### 5.2.1) Query Request\n",
    "> For each period of the day identifies the number of transactions that occurred in that period, and the average number of fraudulent transactions\n",
    "\n",
    "- “For each period of the day”: The query result must contain 4 rows, one for each possible value of `Make_transaction.tx_day_period`. Since the detection of fraudulent transactions for a given month relies on data from the previous month (as seen in query B), it is practical to run this query only considering transactions executed after a specified `startMonthYear` and, for completeness, before a given `endMonthYear`. In this way, if a `startMonthYear` is provided and there are data in the database from the previous month, it becomes possible to calculate the fraudulent transactions for transactions with the same `tx_date_year` and `tx_date_month` as those expressed by `startMonthYear`. If the `startMonthYear` is not provided, it would always be impossible to detect fraudulent transactions for the first month and first year transactions in the database because there would be no data available from the preceding month. If it is not possible to calculate fraudulent transactions for a month, they will be included as 0 in the average calculation.\n",
    "\n",
    "- “the number of transactions”: This means that for each `Make_transaction.tx_day_period`, you need to count the number of transactions registered after `startMonthYear` and before `endMonthYear`.\n",
    "\n",
    "- “the average number of fraudulent transactions”: means calculating the average **montly** count of fraudulent transactions registered after `startMonthYear` and before `endMonthYear` for each desired `Make_transaction.tx_day_period`.\"\n",
    "\n",
    "#### 5.2.2) E1 query code \n",
    "The query starts by setting the `startDate` and `endDate` variables to the first day of the month and year of the Python variables `startDate` and `endMonthYear`, each of which contains a date in the format yyyy-MM. If the Python variables are empty strings, the corresponding query variables are set to `NULL'. This ensures that they are not used to filter the data in the subsequent `WHERE' clause. This approach allows the interval to be partially or completely unspecified, which addresses the previously described problem of fraudulent transactions appearing early in the database records.  \n",
    "\n",
    "The first `MATCH` clause extracts all transactions and the subsequent `WHERE` clause filters these transactions, keeping only those within the specified interval and storing them in the `tx` variable.  \n",
    "\n",
    "The next `WITH` aggregates the transactions in `tx` based on the triple (`tx.tx_date_year`, `tx.tx_date_month`, terminal) and calculates the `tx_amount_fraud_limit` for each of these tuples. Note that the grouping does not use the year and month directly, but rather their associated date value, using the first day of the month incremented by one month. This is because the `tx_amount_fraud_limit' needs to be calculated based on transactions from the previous month, so the `tx_amount_fraud_limit' values we calculate are for the following month.\n",
    "\n",
    "At this stage we have the `tx_amount_fraud_limit` for each triple (`tx.tx_date_year`, `tx.tx_date_month`, terminal). Therefore, we can proceed to count the total number of transactions and the fraudulent transactions associated with each daily period and store them in the variables `tx_count` and `tx_fraud_count` respectively. To achieve this, we use a second `MATCH` clause to extract the transactions corresponding to the same terminal and we filter them using the `WHERE` clause, keeping only those transactions with the same year and month as in the triple, storing them in the variable `tx_current_month`. Then, using the `WITH` clause, we group by the quadruple (`tx.tx_date_year`, `tx.tx_date_month`, terminal, `tx_current_month. tx_day_period`), counting the number of transactions in the `tx_count` variable and also counting the number of fraudulent transactions, defined as those where `tx_current_month.tx_amount > tx_amount_fraud_limit`, and storing the result in the `tx_fraud_count` variable.\n",
    "\n",
    "Finally, the `RETURN` clause aggregates the data by day period only, summing the `tx_count` values into `total_transactions` and calculating the average of the `tx_fraud_count` values as `monthly_avg_fraud_transactions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9990193d-516d-4fcc-a6ab-07178ee1909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_e1 execution time: 0.79s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_period</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>monthly_avg_fraud_transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [day_period, total_transactions, monthly_avg_fraud_transactions]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#startMonthYear is a string that contains an year and a month in the format yyyy-MM, it could be \"\" to not filter the results from a starting point\n",
    "#endMonthYear is a string that contains an year and a month in the format yyyy-MM, it could be \"\" to not filter the results from an ending point\n",
    "#the filtering is [startMonthYear, endMonthYear]\n",
    "def query_e1(startMonthYear, endMonthYear):\n",
    "    query = f\"\"\"\n",
    "            WITH \n",
    "            CASE \n",
    "                WHEN \"{startMonthYear}\" = \"\" THEN NULL\n",
    "                ELSE date(\"{startMonthYear}\" + \"-01\")\n",
    "            END AS startDate,\n",
    "            CASE \n",
    "                WHEN \"{endMonthYear}\" = \"\" THEN NULL\n",
    "                ELSE date(\"{endMonthYear}\" + \"-01\")\n",
    "            END AS endDate\n",
    "            \n",
    "            MATCH (:Customer)-[tx:Make_transaction]->(t:Terminal)\n",
    "            WHERE \n",
    "                 (startDate IS NULL OR (tx.tx_date_year >= startDate.year OR (tx.tx_date_year = startDate.year AND tx.tx_date_month >= startDate.month))) AND\n",
    "                 (endDate IS NULL OR (tx.tx_date_year <= endDate.year OR (tx.tx_date_year = endDate.year AND tx.tx_date_month <= endDate.month)))\n",
    "\n",
    "            WITH (date({{year: tx.tx_date_year, month: tx.tx_date_month, day: 1}}) + duration({{months: 1}})).year AS year, \n",
    "                 (date({{year: tx.tx_date_year, month: tx.tx_date_month, day: 1}}) + duration({{months: 1}})).month AS month, \n",
    "                 t,\n",
    "                 max(tx.tx_amount) * 1.2 as tx_amount_fraud_limit\n",
    "\n",
    "            MATCH (:Customer)-[tx_current_month:Make_transaction]->(t)\n",
    "            WHERE \n",
    "                tx_current_month.tx_date_month = month AND\n",
    "                tx_current_month.tx_date_year = year\n",
    "\n",
    "            WITH \n",
    "                year, \n",
    "                month,\n",
    "                t,\n",
    "                tx_current_month.tx_day_period as day_period,\n",
    "                count(tx_current_month) as tx_count, \n",
    "                count( \n",
    "                    CASE \n",
    "                        WHEN tx_current_month.tx_amount > tx_amount_fraud_limit THEN 1 \n",
    "                        ELSE NULL \n",
    "                    END\n",
    "                )AS tx_fraud_count\n",
    "\n",
    "            RETURN day_period, sum(tx_count) AS total_transactions, avg(tx_fraud_count) AS monthly_avg_fraud_transactions \n",
    "            \"\"\"\n",
    "   \n",
    "    return execute_query_df(\"query_e1\",query)\n",
    "\n",
    "query_e1(\"2023-01\" , month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ed29a-bc45-4708-a59c-3ca0a7747658",
   "metadata": {},
   "source": [
    "#### 5.2.1) E1 Performances\n",
    "This query is the most computationally intensive of the whole workload, as it potentially operates on all the relationships (if no interva is defined) of all the terminals in the DB, and we are not using an optimised and convenient APOC function. Roughly speaking, we can say that it is like running query B for each terminal and for each year and month within the defined interval, then grouping the data by `day_period` and performing the necessary counts and averages. \n",
    "\n",
    "During the development of this query, I expected it to leverage the same composite index created to optimize query A, given that the filtering of transactions is done by breaking down `startDate` and `endDate` into their year and month components: `tx.tx_date_year >= startDate.year AND tx.tx_date_month >= startDate.month` and `tx.tx_date_year <= endDate.year AND tx.tx_date_month <= endDate.month`. However, after reviewing the execution plan, as shown below, this is not the case. This is due to the fact that, in the condition, we check if `startDate` and `endDate` are `NULL`, and in those cases, the filter is not applied.\n",
    "\n",
    "\n",
    "<img src=\"./assets/Execution plan query E1.svg\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49dce8-8825-4fb9-96ec-305e233e1ec5",
   "metadata": {},
   "source": [
    "#### 5.2.2) E2 query code \n",
    "By removing the possibility of setting `startDate` and `endDate` to `NULL` and instead enforcing the definition of an interval, we can take advantage of the composite index we discussed earlier. This would allow the query to efficiently filter transactions based on the `tx.tx_date_year` and `tx.tx_date_month` fields, which are indexed in the composite index, improving performance and making the filtering process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be7f2ac2-cdd8-4821-84ea-91d01b15d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_e2 execution time: 0.53s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_period</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>monthly_avg_fraud_transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [day_period, total_transactions, monthly_avg_fraud_transactions]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#startMonthYear is a string that contains an year and a month in the format yyyy-MM\n",
    "#endMonthYear is a string that contains an year and a month in the format yyyy-MM\n",
    "#the filtering is [startMonthYear, endMonthYear]\n",
    "def query_e2(startMonthYear, endMonthYear):\n",
    "    query = f\"\"\"\n",
    "            WITH \n",
    "            CASE \n",
    "                WHEN \"{startMonthYear}\" = \"\" THEN NULL\n",
    "                ELSE date(\"{startMonthYear}\" + \"-01\")\n",
    "            END AS startDate,\n",
    "            CASE \n",
    "                WHEN \"{endMonthYear}\" = \"\" THEN NULL\n",
    "                ELSE date(\"{endMonthYear}\" + \"-01\")\n",
    "            END AS endDate\n",
    "            \n",
    "            MATCH (:Customer)-[tx:Make_transaction]->(t:Terminal)\n",
    "            WHERE \n",
    "                 (tx.tx_date_year >= startDate.year OR ( tx.tx_date_year = startDate.year AND tx.tx_date_month >= startDate.month)) AND\n",
    "                 (tx.tx_date_year <= endDate.year OR ( tx.tx_date_year = endDate.year AND tx.tx_date_month <= endDate.month))\n",
    "\n",
    "            WITH (date({{year: tx.tx_date_year, month: tx.tx_date_month, day: 1}}) + duration({{months: 1}})).year AS year, \n",
    "                 (date({{year: tx.tx_date_year, month: tx.tx_date_month, day: 1}}) + duration({{months: 1}})).month AS month, \n",
    "                 t,\n",
    "                 max(tx.tx_amount) * 1.2 as tx_amount_fraud_limit\n",
    "\n",
    "            MATCH (:Customer)-[tx_current_month:Make_transaction]->(t)\n",
    "            WHERE \n",
    "                tx_current_month.tx_date_month = month AND\n",
    "                tx_current_month.tx_date_year = year\n",
    "\n",
    "            WITH \n",
    "                year, \n",
    "                month,\n",
    "                t,\n",
    "                tx_current_month.tx_day_period as day_period,\n",
    "                count(tx_current_month) as tx_count, \n",
    "                count( \n",
    "                    CASE \n",
    "                        WHEN tx_current_month.tx_amount > tx_amount_fraud_limit THEN 1 \n",
    "                        ELSE NULL \n",
    "                    END\n",
    "                )AS tx_fraud_count\n",
    "\n",
    "            RETURN day_period, sum(tx_count) AS total_transactions, avg(tx_fraud_count) AS monthly_avg_fraud_transactions \n",
    "            \"\"\"\n",
    "   \n",
    "    return execute_query_df(\"query_e2\",query)\n",
    "\n",
    "query_e2(\"2023-01\" , month_and_year_under_analesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689744e-c10c-4fc6-9bb7-a5bc4d6c2f46",
   "metadata": {},
   "source": [
    "#### 5.2.3) E2 performances\n",
    "From the execution plan shown below, we can see that the composite index is now being used. \n",
    "\n",
    "<img src=\"./assets/Execution plan query E2.svg\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b7680-da81-486d-8e21-886848afcc0e",
   "metadata": {},
   "source": [
    "## 6) Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879299a-31d2-4e0c-b2de-2864e0285db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
