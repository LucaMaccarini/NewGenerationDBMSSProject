{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fcd6c3",
   "metadata": {},
   "source": [
    "# New generation datamodels and DBMSS Project\n",
    "2023 / april 2025 edition\n",
    "\n",
    "Before diving into reading this notebook, make sure you have read the project guidelines provided by the professor. You can find them [here](./Project2023-vers1.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e01d1-ffc7-4f78-bf43-acbbe19cb409",
   "metadata": {},
   "source": [
    "## 1) Transaction Data Simulator Tool\n",
    "\n",
    "This section does not focus on explaining the functionality of the Python scripts or the meaning of the data generated by the tool, as these aspects are clearly detailed on the [linked page](https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_3_GettingStarted/SimulatedDataset.html). Instead, we will focus on how the various scripts provided were combined to produce a single, versatile script. This unified script, through the use of parameters, is capable of generating CSV files containing all the data that will be inserted into the database. \n",
    "\n",
    "To proceed with this section, the following Python packages and Python sources (from this project's repository) are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "8d5306d3-a84f-42dd-ab03-e115c192b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../GenerationScript/Transaction_data_simulator_code'))\n",
    "from add_frauds import add_frauds\n",
    "from generate_dataset import generate_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab3f3b",
   "metadata": {},
   "source": [
    "### 1.1) Parameters\n",
    "\n",
    "To manage parameters for the script in a simple way, I decided to use an array of objects. Each object represents the entire configuration for generating a single database, allowing the script to generate multiple databases with different characteristics and data volumes in one execution.\n",
    "\n",
    "Each object in the array contains configuration data specific to the individual database to be generated, including:\n",
    "\n",
    "- DB_name: The name of the database.\n",
    "- n_customers: The number of customers to generate.\n",
    "- n_terminals: The number of terminals to generate.\n",
    "- start_date: The starting date for generating transaction dates.\n",
    "- n_days: The number of days after the start_date to use for creating transaction dates.\n",
    "- radius: The action radius for customers. A customer can only perform transactions at a terminal within their radius.\n",
    "\n",
    "Here is an example configuration array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d623f6c9",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "DBs = [\n",
    "   {\n",
    "       \"DB_name\": \"Small-DB-1\",\n",
    "       \"n_customers\": 500,\n",
    "       \"n_terminals\": 300,\n",
    "       \"n_days\": 7,\n",
    "       \"start_date\": '2024-12-30',\n",
    "       \"radius\": 10\n",
    "    },\n",
    "    {\n",
    "        \"DB_name\": \"Small-DB-2\",\n",
    "        \"n_customers\": 1000,\n",
    "        \"n_terminals\": 500,\n",
    "        \"n_days\": 14,\n",
    "        \"start_date\": '2024-01-01',\n",
    "        \"radius\": 5\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e01739-8ad8-4b0d-b4c4-479deec0a9e2",
   "metadata": {},
   "source": [
    "### 1.2) Generation Script\n",
    "\n",
    "Below is the code for generating the databases using the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4744055c-ee97-4bd0-a459-6dd87dbbe7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate customer profiles table: 0.01s\n",
      "Time to generate terminal profiles table: 0.00s\n",
      "Time to associate terminals to customers: 0.06s\n",
      "Time to generate transactions: 0.81s\n",
      "Number of frauds from scenario 1: 1\n",
      "Number of frauds from scenario 2: 127\n",
      "Number of frauds from scenario 3: 46\n",
      "Database data saved in: C:\\Users\\luca.maccarini\\Desktop\\luca\\NewGenerationDBMSSProject\\Generated_DBs\\Small-DB-1/\n",
      "\n",
      "Time to generate customer profiles table: 0.02s\n",
      "Time to generate terminal profiles table: 0.00s\n",
      "Time to associate terminals to customers: 0.17s\n",
      "Time to generate transactions: 2.01s\n",
      "Number of frauds from scenario 1: 16\n",
      "Number of frauds from scenario 2: 852\n",
      "Number of frauds from scenario 3: 210\n",
      "Database data saved in: C:\\Users\\luca.maccarini\\Desktop\\luca\\NewGenerationDBMSSProject\\Generated_DBs\\Small-DB-2/\n",
      "\n",
      "DONE! All DBs have been created\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"\"\n",
    "# Loop sui DB definiti nel file di configurazione\n",
    "for db in DBs:\n",
    "    # Generazione delle tabelle del DB usando i valori di configurazione\n",
    "    (customer_profiles_table, terminal_profiles_table, transactions_df) = generate_dataset(\n",
    "        n_customers=db[\"n_customers\"], \n",
    "        n_terminals=db[\"n_terminals\"], \n",
    "        nb_days=db[\"n_days\"], \n",
    "        start_date=db[\"start_date\"], \n",
    "        r=db[\"radius\"]\n",
    "    )\n",
    "\n",
    "    # Aggiungere frodi alle transazioni\n",
    "    transactions_df = add_frauds(customer_profiles_table, terminal_profiles_table, transactions_df)\n",
    "\n",
    "    \n",
    "    # Converto i valori della serie available_terminals dato che gli interi nella lista sono interi numpy\n",
    "    customer_profiles_table['available_terminals'] = customer_profiles_table['available_terminals'].apply(\n",
    "        lambda lst: [int(i) if isinstance(i, np.integer) else i for i in lst] if isinstance(lst, (list, np.array)) else lst\n",
    "    )\n",
    "\n",
    "    # Preparazione al salvataggio del DB\n",
    "    output_dir = os.path.join(os.getcwd(), '..', 'Generated_DBs', db[\"DB_name\"])\n",
    "\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Salvataggio dei customers\n",
    "    customer_profiles_table.to_csv(output_dir + '/customers.csv', sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    # Salvataggio dei terminals\n",
    "    terminal_profiles_table.to_csv(output_dir + '/terminals.csv', sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    # Salvataggio delle transactions\n",
    "    transactions_df.to_csv(output_dir + '/transactions.csv', sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    print(f\"Database data saved in: {os.path.abspath(output_dir)}/\\n\")\n",
    "\n",
    "\n",
    "print(\"DONE! All DBs have been created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7664f-110a-483a-b745-cf45712a853d",
   "metadata": {},
   "source": [
    "### 1.3) CSV Generati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48565d8-453d-4bc9-a6a1-219b0c0fc01c",
   "metadata": {},
   "source": [
    "#### Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "bcb212b5-92c1-46e0-ada7-dcdf9339fade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_customer_id</th>\n",
       "      <th>y_customer_id</th>\n",
       "      <th>mean_amount</th>\n",
       "      <th>std_amount</th>\n",
       "      <th>mean_nb_tx_per_day</th>\n",
       "      <th>available_terminals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.881350</td>\n",
       "      <td>71.518937</td>\n",
       "      <td>62.262521</td>\n",
       "      <td>31.131260</td>\n",
       "      <td>2.179533</td>\n",
       "      <td>[29, 87, 144, 241, 330]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.365480</td>\n",
       "      <td>64.589411</td>\n",
       "      <td>46.570785</td>\n",
       "      <td>23.285393</td>\n",
       "      <td>3.567092</td>\n",
       "      <td>[5, 160, 242, 378, 431, 475]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.366276</td>\n",
       "      <td>38.344152</td>\n",
       "      <td>80.213879</td>\n",
       "      <td>40.106939</td>\n",
       "      <td>2.115580</td>\n",
       "      <td>[316, 406, 447]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.804456</td>\n",
       "      <td>92.559664</td>\n",
       "      <td>11.748426</td>\n",
       "      <td>5.874213</td>\n",
       "      <td>0.348517</td>\n",
       "      <td>[65, 94, 113, 364, 401, 433, 485]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.021840</td>\n",
       "      <td>83.261985</td>\n",
       "      <td>78.924891</td>\n",
       "      <td>39.462446</td>\n",
       "      <td>3.480049</td>\n",
       "      <td>[372]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>43.216661</td>\n",
       "      <td>36.225882</td>\n",
       "      <td>58.023111</td>\n",
       "      <td>29.011555</td>\n",
       "      <td>0.305376</td>\n",
       "      <td>[264, 309, 395, 412, 483]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4.034602</td>\n",
       "      <td>51.110309</td>\n",
       "      <td>7.707631</td>\n",
       "      <td>3.853816</td>\n",
       "      <td>0.238208</td>\n",
       "      <td>[32, 83, 194, 239, 280]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>37.634146</td>\n",
       "      <td>6.177907</td>\n",
       "      <td>41.619615</td>\n",
       "      <td>20.809807</td>\n",
       "      <td>0.967002</td>\n",
       "      <td>[147, 148, 185, 413]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>18.777030</td>\n",
       "      <td>40.467983</td>\n",
       "      <td>16.390871</td>\n",
       "      <td>8.195436</td>\n",
       "      <td>1.398557</td>\n",
       "      <td>[138]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>43.487363</td>\n",
       "      <td>83.000295</td>\n",
       "      <td>93.616587</td>\n",
       "      <td>46.808294</td>\n",
       "      <td>1.233354</td>\n",
       "      <td>[119, 471, 486]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_customer_id  y_customer_id  mean_amount  std_amount  \\\n",
       "CUSTOMER_ID                                                          \n",
       "0                54.881350      71.518937    62.262521   31.131260   \n",
       "1                42.365480      64.589411    46.570785   23.285393   \n",
       "2                96.366276      38.344152    80.213879   40.106939   \n",
       "3                56.804456      92.559664    11.748426    5.874213   \n",
       "4                 2.021840      83.261985    78.924891   39.462446   \n",
       "...                    ...            ...          ...         ...   \n",
       "995              43.216661      36.225882    58.023111   29.011555   \n",
       "996               4.034602      51.110309     7.707631    3.853816   \n",
       "997              37.634146       6.177907    41.619615   20.809807   \n",
       "998              18.777030      40.467983    16.390871    8.195436   \n",
       "999              43.487363      83.000295    93.616587   46.808294   \n",
       "\n",
       "             mean_nb_tx_per_day                available_terminals  \n",
       "CUSTOMER_ID                                                         \n",
       "0                      2.179533            [29, 87, 144, 241, 330]  \n",
       "1                      3.567092       [5, 160, 242, 378, 431, 475]  \n",
       "2                      2.115580                    [316, 406, 447]  \n",
       "3                      0.348517  [65, 94, 113, 364, 401, 433, 485]  \n",
       "4                      3.480049                              [372]  \n",
       "...                         ...                                ...  \n",
       "995                    0.305376          [264, 309, 395, 412, 483]  \n",
       "996                    0.238208            [32, 83, 194, 239, 280]  \n",
       "997                    0.967002               [147, 148, 185, 413]  \n",
       "998                    1.398557                              [138]  \n",
       "999                    1.233354                    [119, 471, 486]  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(os.path.join(output_dir, 'customers.csv'), sep=';', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29e7a5-d9b6-4e4b-8ef4-54d84a9ed8ee",
   "metadata": {},
   "source": [
    "#### Terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3dfd3649-9afb-4440-9863-4fb1eadeba66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_terminal_id</th>\n",
       "      <th>y_terminal_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.702200</td>\n",
       "      <td>72.032449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011437</td>\n",
       "      <td>30.233257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.675589</td>\n",
       "      <td>9.233859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.626021</td>\n",
       "      <td>34.556073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.676747</td>\n",
       "      <td>53.881673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>81.250730</td>\n",
       "      <td>28.380183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>52.784680</td>\n",
       "      <td>33.941672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>55.466731</td>\n",
       "      <td>97.440347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>31.170292</td>\n",
       "      <td>66.879661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>32.596721</td>\n",
       "      <td>77.447727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_terminal_id  y_terminal_id\n",
       "TERMINAL_ID                              \n",
       "0                41.702200      72.032449\n",
       "1                 0.011437      30.233257\n",
       "2                14.675589       9.233859\n",
       "3                18.626021      34.556073\n",
       "4                39.676747      53.881673\n",
       "...                    ...            ...\n",
       "495              81.250730      28.380183\n",
       "496              52.784680      33.941672\n",
       "497              55.466731      97.440347\n",
       "498              31.170292      66.879661\n",
       "499              32.596721      77.447727\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(output_dir, 'terminals.csv'), sep=';', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338c7ff-e76e-4a56-a4fd-daae427cb364",
   "metadata": {},
   "source": [
    "#### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f2384961-3d3b-40d4-b7c3-7e0a713ab315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_TIME_SECONDS</th>\n",
       "      <th>TX_TIME_DAYS</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_FRAUD_SCENARIO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-01 00:00:31</td>\n",
       "      <td>596</td>\n",
       "      <td>110</td>\n",
       "      <td>57.16</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-01 00:07:56</td>\n",
       "      <td>2</td>\n",
       "      <td>316</td>\n",
       "      <td>146.00</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-01 00:10:34</td>\n",
       "      <td>927</td>\n",
       "      <td>415</td>\n",
       "      <td>50.99</td>\n",
       "      <td>634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-01 00:10:45</td>\n",
       "      <td>568</td>\n",
       "      <td>400</td>\n",
       "      <td>44.71</td>\n",
       "      <td>645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-01 00:13:44</td>\n",
       "      <td>541</td>\n",
       "      <td>171</td>\n",
       "      <td>59.07</td>\n",
       "      <td>824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25206</th>\n",
       "      <td>2018-04-14 23:45:57</td>\n",
       "      <td>786</td>\n",
       "      <td>104</td>\n",
       "      <td>40.75</td>\n",
       "      <td>1208757</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25207</th>\n",
       "      <td>2018-04-14 23:46:06</td>\n",
       "      <td>392</td>\n",
       "      <td>134</td>\n",
       "      <td>7.15</td>\n",
       "      <td>1208766</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25208</th>\n",
       "      <td>2018-04-14 23:46:40</td>\n",
       "      <td>888</td>\n",
       "      <td>141</td>\n",
       "      <td>37.55</td>\n",
       "      <td>1208800</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25209</th>\n",
       "      <td>2018-04-14 23:51:33</td>\n",
       "      <td>347</td>\n",
       "      <td>64</td>\n",
       "      <td>23.28</td>\n",
       "      <td>1209093</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25210</th>\n",
       "      <td>2018-04-14 23:56:53</td>\n",
       "      <td>164</td>\n",
       "      <td>212</td>\n",
       "      <td>66.56</td>\n",
       "      <td>1209413</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25211 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  \\\n",
       "TRANSACTION_ID                                                             \n",
       "0               2018-04-01 00:00:31          596          110      57.16   \n",
       "1               2018-04-01 00:07:56            2          316     146.00   \n",
       "2               2018-04-01 00:10:34          927          415      50.99   \n",
       "3               2018-04-01 00:10:45          568          400      44.71   \n",
       "4               2018-04-01 00:13:44          541          171      59.07   \n",
       "...                             ...          ...          ...        ...   \n",
       "25206           2018-04-14 23:45:57          786          104      40.75   \n",
       "25207           2018-04-14 23:46:06          392          134       7.15   \n",
       "25208           2018-04-14 23:46:40          888          141      37.55   \n",
       "25209           2018-04-14 23:51:33          347           64      23.28   \n",
       "25210           2018-04-14 23:56:53          164          212      66.56   \n",
       "\n",
       "                TX_TIME_SECONDS  TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n",
       "TRANSACTION_ID                                                              \n",
       "0                            31             0         0                  0  \n",
       "1                           476             0         0                  0  \n",
       "2                           634             0         0                  0  \n",
       "3                           645             0         0                  0  \n",
       "4                           824             0         0                  0  \n",
       "...                         ...           ...       ...                ...  \n",
       "25206                   1208757            13         0                  0  \n",
       "25207                   1208766            13         0                  0  \n",
       "25208                   1208800            13         0                  0  \n",
       "25209                   1209093            13         0                  0  \n",
       "25210                   1209413            13         0                  0  \n",
       "\n",
       "[25211 rows x 8 columns]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(output_dir, 'transactions.csv'), sep=';', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd624c-46c3-4f07-a8b8-f3e58796b4f2",
   "metadata": {},
   "source": [
    "## 2) Conceptual Model\n",
    "\n",
    "To create the following conceptual model, I analyzed the data generated by the \"Transaction Data Simulator\" tool, aiming to understand its semantics in order to design a simple structure that clearly illustrates the relationships between the data to be stored in the database. Additionally, I grouped some data into custom types to further enhance the semantics and readability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7225c-f30e-4129-aa42-5f9837bd76d4",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Conceptual model UML.svg\" alt=\"UML Diagram\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9b0af-a1db-4aad-b8ce-ab071041fd14",
   "metadata": {},
   "source": [
    "### 2.2) Costraints\n",
    "#### Terminal\n",
    "- 0 <= `coords.x` <= 100\n",
    "- 0 <= `coords.y` <= 100\n",
    "\n",
    "#### Customer\n",
    "- 0 <= `coords.x` <= 100\n",
    "- 0 <= `coords.y` <= 100\n",
    "- `spending_mean` >= 0\n",
    "- `spending_std` >= 0\n",
    "- `transactions_per_day_mean` >= 0\n",
    "\n",
    "#### Transactions\n",
    "- `amount` > 0\n",
    "- 0 <= `fraud_scenario` <= 3\n",
    "- 0 <= `security_feeling` <= 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cadaf-07f9-4964-8509-e3bed43d7c1c",
   "metadata": {},
   "source": [
    "## 3) Logical Model\n",
    "\n",
    "Before proceeding with the logical model, it is important to indicate which database I have chosen to manage the data and the decisions I made regarding the representation of the data to meet the workload requirements.\n",
    "\n",
    "### 3.1) Database\n",
    "As a database, I chose to use Neo4j due to the nature of the data, which suggests a graph structure. In fact, all the relationships present are of the N:N type, and such relationships are excellently handled by graph databases. \n",
    "\n",
    "Furthermore, this choice was confirmed by the workload, especially by query 3c, which involves continuous traversal of relationships up to a certain `K` value that determines when to stop. Performing this query would be extremely costly if we had to perform a join (or lookup) for each traversed relationship. \n",
    "\n",
    "Additionally, as we will see later, Cypher, Neo4j's query language, offers a library called APOC that will allow us to execute query 3c with impressive performance.\n",
    "\n",
    "### 3.2) Data representation (Workload friendly)\n",
    "Since Neo4j does not allow the definition of custom types or the insertion of objects within node properties, I decided to eliminate all custom types and implement them using primitive types. For the custom types representing objects, I created a property for each attribute with its corresponding primitive type. For enums, I used simple strings.\n",
    "\n",
    "The attribute names in the logical model differ from those in the conceptual model because they are the same as those used by the \"Transaction Data Simulator\" tool, except for the new data added by me, which are the ones explained in the following paragraph or the ones that were to be added as indicated in the project guidelines. For more information on what a specific field means, refer to the page provided in the project guidelines on the \"Transaction Data Simulator\" tool, as it explains all the fields in detail.\n",
    "\n",
    "As we will see later, to improve the efficiency of the workload through indexing, I decided to split the `transactions.registration` field into its components: day, month, year, and time. These components are now represented as `tx_date_day`, `tx_date_month`, `tx_date_year`, and `tx_date_time`, respectively. This division was made because many queries in the workload filter data using only the month and year of the transactions.registration field. If I had created an index on the entire field, it would not have been used, as the filters in the queries would only utilize a subset of the entire field. Therefore, the division was made, and a composite index was created only on the year and month fields.\n",
    "\n",
    "The data types specified are those present in Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff40efb-95e5-4285-a131-1b0e61bf7a93",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Logical model UML.svg\" alt=\"UML Diagram\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c04e6-2e07-463d-af89-5863a1ed487a",
   "metadata": {},
   "source": [
    "### 3.3) Costraints\n",
    "#### Terminal\n",
    "- 0 <= `x_terminal_id` <= 100\n",
    "- 0 <= `y_terminal_id` <= 100\n",
    "\n",
    "#### Customer\n",
    "- 0 <= `x_customer_id` <= 100\n",
    "- 0 <= `y_customer_id` <= 100\n",
    "- `mean_amount` >= 0\n",
    "- `std_amount` >= 0\n",
    "- `mean_nb_tx_per_day` >= 0\n",
    "\n",
    "#### Transactions\n",
    "- `tx_amount` > 0\n",
    "- 0 <= `tx_fraud_scenario` <= 3\n",
    "- 0 <= `tx_security_feeling` <= 5\n",
    "- `tx_date_day`, `tx_date_month`, `tx_date_year` form a correct date type object \n",
    "- `tx_date_time` forms a correct localTime object\n",
    "- `tx_day_period` is one of the following strings [\"morning\", \"afternoon\", \"evening\", \"night\"]\n",
    "- `tx_products_type` is one of the following strings [\"high-tech\", \"food\", \"clothing\", \"consumable\", \"other\"]\n",
    "\n",
    "### 3.4) Assumptions\n",
    "Since the constraints that can be implemented in Neo4j focus only on the structure and data type, and do not allow constraints on the actual values or the direction of relationships, I assume that whichever software provides the data to be inserted into the database has correctly implemented all the constraints listed above (except for the constraints on the properties `tx_date_...`, since those can be validated at the database level). In our case, we assume that the values produced by the \"Transaction Data Simulator\" tool are correct and comply with the constraints. \n",
    "\n",
    "Since Neo4j constraints also do not allow us to define the direction of relationships, it is our responsibility to ensure that, in the queries used to create relationships, we do not make mistakes and avoid generating relationships in the wrong direction.\n",
    "\n",
    "For more detailed information, I refer you to the Neo4j [documentation](https://neo4j.com/docs/cypher-manual/current/constraints/managing-constraints/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39af186-d0a2-4bf5-b597-b8f5a350bc51",
   "metadata": {},
   "source": [
    "## 4) Neo4j Data Loading\n",
    "To proceed with this section, the following Python packages are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "95383e31-f7c8-4b24-a4e9-0facf81aecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7822175-8e8e-4ac4-93da-66f667091953",
   "metadata": {},
   "source": [
    "To facilitate interactions with Neo4j, we will define some \"kernel\" functions that will be used to interface with the database. These functions will simplify managing data with Neo4j, providing reusable methods for the rest of the project.\n",
    "\n",
    "Before defining the kernel functions, we set some configuration parameters that will be useful not only for the kernel functions themselves but also for the various queries that will be executed later in the project through the kernel functions.\n",
    "Among the configuration parameters, we have:\n",
    "- `customers_csv_link`, `terminals_csv_link`, `transactions_csv_link`: these are references to the previously generated CSV files. They can be local paths or network links, and we will explain in the appropriate section why we use network links. The files refer to the example where **the total size of the three files is 100 MB**, but in the section discussing performance, we will also show load times for both larger and smaller databases.\n",
    "- `lines_per_commit`: useful for batch operations sent to the database through specific Cypher directives (in our case, we will use APOC). This parameter indicates how many modified or added rows should be processed before committing the data.\n",
    "- `parallel_loading`: useful for the batch operations mentioned in the previous point. This parameter indicates whether the database should perform the batch operations in parallel or sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c58278ee-178d-4b60-b1ff-88abd5045d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parameters\n",
    "config = {\n",
    "    \"customers_csv_link\": \"https://www.dropbox.com/scl/fi/etkusr9d6rr7lrc32zxre/customers.csv?rlkey=l3pzkk66au5xuyshhhk392497&st=ue72fvro&dl=1\",\n",
    "    \"terminals_csv_link\": \"https://www.dropbox.com/scl/fi/j9rbktdvvujcc3jmnovc8/terminals.csv?rlkey=jesdwacp0tmgwqzsuyycwgvb5&st=gxa7tbro&dl=1\",\n",
    "    \"transactions_csv_link\": \"https://www.dropbox.com/scl/fi/nr8jg8ofwh22geyx6vsrv/transactions.csv?rlkey=ftc258g3frk4unri1icqmhdrg&st=iskbq8ci&dl=1\",\n",
    "    \"lines_per_commit\": 1000,\n",
    "    \"parallel_loading\": \"true\"\n",
    "}\n",
    "\n",
    "# provide neo4j db connection\n",
    "def get_neo4j_connection():\n",
    "    try:\n",
    "        #Using environment variables (recommended): This method securely stores credentials outside the code by using environment variables.\n",
    "        #uri = os.getenv('NEO4J_URI')\n",
    "        #user = os.getenv('NEO4J_USERNAME')\n",
    "        #password = os.getenv('NEO4J_PASSWORD')\n",
    "        \n",
    "        #Using plain strings (not recommended): This method directly includes credentials in the code, which exposes them to potential security risks.\n",
    "        #In this case, to keep things as simple as possible, I will use plain text credentials since they are for a free version of Neo4j.\n",
    "        #You can create it by following this link: https://neo4j.com/product/auradb\n",
    "        uri = \"neo4j+s://45d4bc57.databases.neo4j.io\"\n",
    "        user = \"neo4j\"\n",
    "        password = \"o8mbh0hFGILahScLJw2yTYWIwQ6z7lPhQT6m-U2W1c8\"\n",
    "        \n",
    "        return neo4j.GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred while connecting to Neo4j: {e}\")\n",
    "        return None\n",
    "\n",
    "# close neo4j db connection\n",
    "def close_neo4j_connection(driver):\n",
    "    if driver is not None:\n",
    "        driver.close()\n",
    "\n",
    "# clear the db from data, relations and costraints\n",
    "def clear_database():\n",
    "    driver = get_neo4j_connection()\n",
    "    if driver is None:\n",
    "        return False\n",
    "\n",
    "    delete_nodes_query = f\"\"\"\n",
    "        MATCH (n)\n",
    "        CALL apoc.nodes.delete(n, {config[\"lines_per_commit\"]}) YIELD value\n",
    "        RETURN value\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        start_time=time.time()\n",
    "        driver.execute_query(delete_nodes_query)\n",
    "\n",
    "        constraints_result = driver.execute_query(\"SHOW CONSTRAINTS\").records\n",
    "        for record in constraints_result:\n",
    "            drop_constraint_query = \"DROP CONSTRAINT $name\"\n",
    "            driver.execute_query(drop_constraint_query, {\"name\": record[\"name\"]})\n",
    "\n",
    "        indexes_result = driver.execute_query(\"SHOW INDEXES\").records\n",
    "        for record in indexes_result:\n",
    "            drop_index_query = \"DROP INDEX $name\"\n",
    "            driver.execute_query(drop_index_query, {\"name\": record[\"name\"]})\n",
    "\n",
    "        print(\"clear_database execution time: {:.2f}s\".format(time.time()-start_time))\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR clear_database: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)\n",
    "\n",
    "# Performs a query that does not expect data as a result\n",
    "def execute_query_command(name, query):\n",
    "    driver = get_neo4j_connection()\n",
    "    try:\n",
    "        start_time=time.time()\n",
    "        driver.execute_query(query)\n",
    "        print(f\"{name} execution time: {{:.2f}}s\".format(time.time()-start_time))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR {name}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)\n",
    "        \n",
    "# Performs some querys where each one does not expect data as a result\n",
    "def execute_query_commands(name, queries):\n",
    "    driver = get_neo4j_connection()\n",
    "    try:\n",
    "        start_time=time.time()\n",
    "        \n",
    "        for query in queries:\n",
    "            driver.execute_query(query)\n",
    "\n",
    "        print(f\"{name} execution time: {{:.2f}}s\".format(time.time()-start_time))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR {name}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)\n",
    "\n",
    "# performs a query that returns data and converts it to a dataframe\n",
    "def execute_query_df(name, query):\n",
    "    driver = get_neo4j_connection()\n",
    "    if driver is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        start_time=time.time()\n",
    "        result = driver.execute_query(query, result_transformer_= neo4j.Result.to_df)\n",
    "        print(f\"{name} execution time: {{:.2f}}s\".format(time.time() - start_time))\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR {name}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        close_neo4j_connection(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098dadb5-4648-420c-b573-ab23124907dc",
   "metadata": {},
   "source": [
    "**Let’s begin by cleaning the database.** This step is unnecessary if you have just created a new database instance, but if you are reusing an instance on which you have already performed some operations, such as running this notebook before, it is advisable to restore it to its original state by clearing everything. In this case, the `clear_database()` function comes to our aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "73ad3ab8-2b74-46dd-a5ae-f02d0092de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear_database execution time: 16.19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893ebdd-a7d9-4b76-912e-0923b67c4c4f",
   "metadata": {},
   "source": [
    "### 4.1) Schema\n",
    "Neo4j constraints focus solely on the data structure, as they are used to define a schema for the data. Thanks to Neo4j's schemaless nature, or more generally the schemaless nature of NoSQL databases, it is possible to insert data with maximum flexibility, without the need to define a formal schema in advance. This flexibility allows for handling heterogeneous data and adapting to changes over time, making it ideal for scenarios where the data structure may evolve.\n",
    "\n",
    "However, despite this flexibility, defining a schema is still considered good practice. It provides several benefits, particularly in terms of performance when running queries that filter data or when calculations need to be performed on the data. By enforcing data types through the schema, the database can optimize certain operations, especially those that involve processing values. On the other hand, one drawback of using a schema is that it requires additional processing during insertions and modifications, as the database must validate that each new piece of data complies with the defined constraints.\n",
    "\n",
    "The schema we are about to define in the database involves taking the previously documented logical model and:\n",
    "- adding constraints that associate each attribute with its respective type;\n",
    "- defining, for each entity (from the logical model), the attributes that form the primary key.\n",
    "- Adding constraints that make the attributes mandatory, for attributes not specified as primary keys, since they are already mandatory due to the primary key constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "33ffa2bc-6bfb-4fba-a28d-954560379160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_terminals_schema execution time: 1.34s\n",
      "create_customers_schema execution time: 2.00s\n",
      "create_transaction_schema execution time: 2.80s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_terminals_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT terminal_id_is_integer FOR (t:Terminal) REQUIRE t.terminal_id IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT terminal_id_key FOR (t:Terminal) REQUIRE t.terminal_id IS NODE KEY;\",\n",
    "        \"CREATE CONSTRAINT terminal_x_is_float FOR (t:Terminal) REQUIRE t.x_terminal_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT terminal_x_required FOR (t:Terminal) REQUIRE t.x_terminal_id IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT terminal_y_is_float FOR (t:Terminal) REQUIRE t.y_terminal_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT terminal_y_required FOR (t:Terminal) REQUIRE t.y_terminal_id IS NOT NULL;\"\n",
    "    ]\n",
    "    \n",
    "    return execute_query_commands(\"create_terminals_schema\", queries)\n",
    "\n",
    "def create_customers_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT customer_id_is_integer FOR (c:Customer) REQUIRE c.customer_id IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT customer_id_key FOR (c:Customer) REQUIRE c.customer_id IS NODE KEY;\",\n",
    "        \"CREATE CONSTRAINT customer_x_is_float FOR (c:Customer) REQUIRE c.x_customer_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_x_required FOR (c:Customer) REQUIRE c.x_customer_id IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_y_is_float FOR (c:Customer) REQUIRE c.y_customer_id IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_y_required FOR (c:Customer) REQUIRE c.y_customer_id IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_amount_is_float FOR (c:Customer) REQUIRE c.mean_amount IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_amount_required FOR (c:Customer) REQUIRE c.mean_amount IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_std_amount_is_float FOR (c:Customer) REQUIRE c.std_amount IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_std_amount_required FOR (c:Customer) REQUIRE c.std_amount IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_nb_tx_per_day_is_float FOR (c:Customer) REQUIRE c.mean_nb_tx_per_day IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT customer_mean_nb_tx_per_day_required FOR (c:Customer) REQUIRE c.mean_nb_tx_per_day IS NOT NULL;\"\n",
    "    ]\n",
    "    return execute_query_commands(\"create_customers_schema\", queries)\n",
    "\n",
    "def create_transaction_schema():\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT transaction_id_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.transaction_id IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT transaction_id_key FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.transaction_id IS RELATIONSHIP KEY;\",\n",
    "        \"CREATE CONSTRAINT tx_time_seconds_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_seconds IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_time_seconds_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_seconds IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_time_days_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_days IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_time_days_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_time_days IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_amount_is_float FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_amount IS :: FLOAT;\",\n",
    "        \"CREATE CONSTRAINT tx_amount_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_amount IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_day_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_day IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_day_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_day IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_date_month_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_month IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_date_month_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_month IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_year_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_year IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_date_year_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_year IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_date_time_is_localtime FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_time IS :: LOCAL TIME;\",\n",
    "        \"CREATE CONSTRAINT tx_date_time_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_date_time IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_is_boolean FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud IS :: BOOLEAN;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_is_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud IS NOT NULL;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_scenario_is_integer FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud_scenario IS :: INTEGER;\",\n",
    "        \"CREATE CONSTRAINT tx_fraud_scenario_is_required FOR ()-[transaction:Make_transaction]->() REQUIRE transaction.tx_fraud_scenario IS NOT NULL;\"\n",
    "    ]\n",
    "    return execute_query_commands(\"create_transaction_schema\", queries)\n",
    "\n",
    "create_terminals_schema()\n",
    "create_customers_schema()\n",
    "create_transaction_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8776b-b0f4-490f-a03e-747dbd8cc818",
   "metadata": {},
   "source": [
    "### 4.2) Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230cf7a-cfbc-4387-9f0d-e02f4f9cb736",
   "metadata": {},
   "source": [
    "To load data into Neo4j using CSV files, we must first consider where the Neo4j instance resides in which we want to load the data. This aspect is crucial because the CSV files must be accessible from the machine running the Neo4j instance. This results in two possible scenarios:\n",
    "- The CSV files reside on the machine where the Neo4j instance is running,\n",
    "- The CSV files are network resources that can be directly downloaded via a link.\n",
    "Since we are using a Neo4j instance managed by an external company, and they obviously do not provide us access to their servers, we must opt for the second option.\n",
    "\n",
    "This will have an \"impact\" on the data loading performance, as the time indicated by the loading procedure will not only account for the time required to load the data from the file to the database but will also include the time for the Neo4j instance to download the file. The download time is not negligible because, as we know, the network is much slower compared to a completely local approach. Moreover, with the larger CSV files generated earlier and available via the links in the configuration object previously commented, this time should not be underestimated. Try it yourself by pasting the URL in your browser and seeing how long it takes for your machine to download the CSV file.\n",
    "\n",
    "To make the CSV files network resources with a direct download link easily and quickly, I used Dropbox and enabled file sharing. I chose Dropbox because it provides sharing links that incorporate a query parameter in the URL, which allows specifying whether to make the URL a direct download link, which is necessary for the Neo4j instance to download the file correctly. The parameter in question appears at the end of the link as &dl=1; if set to zero, it disables the direct download and opens a web viewer for the file. I also tried other cloud storage systems, but the process to get the direct download link was unnecessarily more complicated.\n",
    "\n",
    "Now let's look at the queries used to load the data into the database. Initially, I wanted to load the data using the same example provided by the professor during the lessons, where a Cypher directive was used to load data from a CSV file in batches of N rows per commit. However, since this directive has been deprecated, I opted to use the APOC library, which allowed me to achieve the same behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "2d33bb27-1736-4e1a-aa16-b2e7d459dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_terminals_from_csv execution time: 2.08s\n",
      "load_customers_with_available_terminals_from_csv execution time: 2.62s\n",
      "load_transactions_from_csv execution time: 29.16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_terminals_from_csv():\n",
    "    query = f\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "            'LOAD CSV WITH HEADERS FROM \"{config[\"terminals_csv_link\"]}\" AS row FIELDTERMINATOR \";\" \n",
    "            RETURN row',\n",
    "            'MERGE (t:Terminal {{terminal_id: toInteger(row.TERMINAL_ID)}})\n",
    "            ON CREATE SET \n",
    "                t.x_terminal_id = toFloat(row.x_terminal_id),\n",
    "                t.y_terminal_id = toFloat(row.y_terminal_id)\n",
    "            ',\n",
    "            {{batchSize: {config[\"lines_per_commit\"]}, parallel: {config[\"parallel_loading\"]}}}\n",
    "        )\n",
    "    \"\"\"\n",
    "    return execute_query_command(\"load_terminals_from_csv\", query)\n",
    "\n",
    "def load_customers_with_available_terminals_from_csv():    \n",
    "    query = f\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "            'LOAD CSV WITH HEADERS FROM \"{config[\"customers_csv_link\"]}\" AS row FIELDTERMINATOR \";\" \n",
    "            RETURN row',\n",
    "            'MERGE (c:Customer {{customer_id: toInteger(row.CUSTOMER_ID)}})\n",
    "            ON CREATE SET  \n",
    "                c.x_customer_id = toFloat(row.x_customer_id),\n",
    "                c.y_customer_id = toFloat(row.y_customer_id),\n",
    "                c.mean_amount = toFloat(row.mean_amount),\n",
    "                c.std_amount = toFloat(row.std_amount),\n",
    "                c.mean_nb_tx_per_day = toFloat(row.mean_nb_tx_per_day)\n",
    "            WITH c, row\n",
    "            WITH c, apoc.convert.fromJsonList(row.available_terminals) AS available_terminal_ids\n",
    "            UNWIND available_terminal_ids AS available_terminal_id\n",
    "            MATCH (t:Terminal {{terminal_id: available_terminal_id}})\n",
    "            MERGE (c)-[:Available]->(t)\n",
    "            ',\n",
    "            {{batchSize: {config[\"lines_per_commit\"]}, parallel: {config[\"parallel_loading\"]}}}\n",
    "        )\n",
    "    \"\"\"\n",
    "    return execute_query_command(\"load_customers_with_available_terminals_from_csv\",query)\n",
    "\n",
    "def load_transactions_from_csv():\n",
    "    query = f\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "            'LOAD CSV WITH HEADERS FROM \"{config[\"transactions_csv_link\"]}\" AS row FIELDTERMINATOR \";\" \n",
    "            RETURN row',\n",
    "            'WITH row,\n",
    "                  split(row.TX_DATETIME, \" \") AS splitted_date_time\n",
    "                  \n",
    "            WITH row,\n",
    "                 date(splitted_date_time[0]) AS parsed_date,\n",
    "                 localtime(splitted_date_time[1]) AS parsed_local_time\n",
    "\n",
    "            MATCH (c:Customer {{customer_id: toInteger(row.CUSTOMER_ID)}}), \n",
    "                (t:Terminal {{terminal_id: toInteger(row.TERMINAL_ID)}})\n",
    "            MERGE (c)-[transaction:Make_transaction {{transaction_id: toInteger(row.TRANSACTION_ID)}}]->(t)\n",
    "            ON CREATE SET \n",
    "                transaction.tx_time_seconds = toInteger(row.TX_TIME_SECONDS), \n",
    "                transaction.tx_time_days = toInteger(row.TX_TIME_DAYS),\n",
    "                transaction.tx_amount = toFloat(row.TX_AMOUNT), \n",
    "                transaction.tx_fraud = toBoolean(toInteger(row.TX_FRAUD)), \n",
    "                transaction.tx_fraud_scenario = toInteger(row.TX_FRAUD_SCENARIO),\n",
    "\n",
    "                transaction.tx_date_day = parsed_date.day,\n",
    "                transaction.tx_date_month = parsed_date.month,\n",
    "                transaction.tx_date_year = parsed_date.year, \n",
    "                transaction.tx_date_time = parsed_local_time \n",
    "            ',\n",
    "            {{batchSize: {config[\"lines_per_commit\"]}, parallel: {config[\"parallel_loading\"]}}}\n",
    "        )\n",
    "    \"\"\"\n",
    "    return execute_query_command(\"load_transactions_from_csv\",query)\n",
    "\n",
    "load_terminals_from_csv()\n",
    "load_customers_with_available_terminals_from_csv()\n",
    "load_transactions_from_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffc76c-18c7-4ddd-a0b5-749390db5388",
   "metadata": {},
   "source": [
    "## 5) Workload\n",
    "In this section, we’ll see how I implemented the queries to respond as efficiently as possible to the various requests in the project specifications. Since the requested queries were not always precise in every detail, the analysis of each query will follow these points:\n",
    "- Report the query as expressed in the project specifications.\n",
    "- Explain my interpretation of the query.\n",
    "- Present the query code, providing a detailed explanation.\n",
    "- Look at the results\n",
    "- Evaluate the query's performance.\n",
    "\n",
    "Additional details on performance will be included in the dedicated section, where the execution times of the various queries will be compared across databases of different sizes.\n",
    "\n",
    "**An important note:** since I couldn’t find a way to clear the caches in the free Neo4j instance (and I don’t think it’s possible), when comparing the execution times of different versions of the same query, or the same query on different DBs, it’s important to ensure the accuracy of the timings by running them multiple times, so we will look at the chached queries timing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e41734-85e9-4fe2-be18-43cadd3afe18",
   "metadata": {},
   "source": [
    "### 5.1) Query A\n",
    "#### 5.1.2) Requested query\n",
    "> For each customer checks that the spending frequency and the spending amounts of the last month is under the usual spending frequency and the spending amounts for the same period.\n",
    "\n",
    "- \"for each customer\": this indicates that the query results must include all customers, even those for whom it is not possible to calculate the requested data.  \n",
    "\n",
    "- \"of the last month\": this indicates that the query should work with data from the previous month. To make the query parameterized, I implemented the possibility to pass a partial date in the \"yyyy-MM\" format to the Python function executing the query. This date is used to calculate a **reference date**, which corresponds to the first day of the month preceding the month of the date passed to the function. For the reference date, only the month and year are considered to filter the relevant data.  \n",
    "\n",
    "- \"usual spending frequency and the spending amounts for the same period\": I interpreted this to mean that the spending frequency and spending amount must be calculated as the average of all spending frequencies and amounts recorded in the database that match the same month but correspond to a year earlier than the reference date.\n",
    "\n",
    "#### 5.1.3) Query code \n",
    "The query starts by calculating the date corresponding to the first day of the previous month relative to the date provided to the Python function. This date is saved in the variable `first_of_previous_month`.\n",
    "\n",
    "Next, all customers are matched to ensure that none are excluded from the final result of the query. This is done because the following `WHERE` clauses do not filter out customers, and all subsequent matches are `OPTIONAL MATCH`.\n",
    "\n",
    "The first `OPTIONAL MATCH` is used to retrieve the transaction history for the same period.\n",
    "\n",
    "The subsequent `WITH` clause is particular because, instead of summing and counting the amount and frequency of past transactions, it returns `NULL` for both if no transactions are found in the history. This is useful for differentiating, in the final result, customers for whom no significant transaction history is found (and therefore no calculations can be made) from those for whom a history is available and calculations can proceed as required by the query.\n",
    "\n",
    "The next `WITH` clause calculates the averages of the results just computed. The `AVG` operator preserves the `NULL` value when calculating based on `NULL`; thus, if there are no transactions, `AVG(NULL)` will return `NULL`.\n",
    "\n",
    "The final `OPTIONAL MATCH` performs the same calculations of the previous one, but this time on transactions that have the same month and year as `first_of_previous_month`, but now in the subsequent `WITH`, we don't differentiate customers based on the presence of data. This because in the `RETURN` clause, where we check that both avg spending frequency and avg spending amount for the previous month transactions are below the usual (historical) ones. If a customer has no transactions for the month and year corresponding to `first_of_previous_month` but has a historical record, it means that they simply didn't spend in the previous month, so their spending frequency and amount will be below the usual threshold. If a customer has neither transactions for the referenced month nor a history, it means no response can be provided for that customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "4d7b78d7-88dd-47ef-8320-94f6655c6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_a1 execution time: 4.01s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>is_under_total_amount_avg_of_same_period</th>\n",
       "      <th>is_under_monthly_freq_avg_of_same_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(mean_amount, x_customer_id, mean_nb_tx_per_da...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    c  \\\n",
       "0   (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "1   (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "2   (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "3   (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "4   (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "..                                                ...   \n",
       "95  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "96  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "97  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "98  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "99  (mean_amount, x_customer_id, mean_nb_tx_per_da...   \n",
       "\n",
       "   is_under_total_amount_avg_of_same_period  \\\n",
       "0                                      True   \n",
       "1                                      True   \n",
       "2                                      None   \n",
       "3                                      True   \n",
       "4                                      None   \n",
       "..                                      ...   \n",
       "95                                     None   \n",
       "96                                     None   \n",
       "97                                     None   \n",
       "98                                     True   \n",
       "99                                     None   \n",
       "\n",
       "   is_under_monthly_freq_avg_of_same_period  \n",
       "0                                      True  \n",
       "1                                      True  \n",
       "2                                      None  \n",
       "3                                     False  \n",
       "4                                      None  \n",
       "..                                      ...  \n",
       "95                                     None  \n",
       "96                                     None  \n",
       "97                                     None  \n",
       "98                                     True  \n",
       "99                                     None  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year_and_month_under_analesis is a string that contains a year and a month in the format yyyy-MM\n",
    "def query_a1(year_and_month_under_analesis):\n",
    "    query = f\"\"\"\n",
    "            WITH date.truncate('month', date(\"{year_and_month_under_analesis}\" + \"-01\") ) - duration({{months: 1}}) AS first_of_previous_month\n",
    "            \n",
    "            MATCH (c:Customer)\n",
    "\n",
    "            OPTIONAL MATCH (c)-[tx_prev_month_all_prev_year:Make_transaction]->(:Terminal)\n",
    "            WHERE \n",
    "                tx_prev_month_all_prev_year.tx_date_month = first_of_previous_month.month\n",
    "                AND tx_prev_month_all_prev_year.tx_date_year < first_of_previous_month.year\n",
    "            WITH\n",
    "                first_of_previous_month,\n",
    "                c,\n",
    "                tx_prev_month_all_prev_year.tx_date_year as year, \n",
    "                CASE \n",
    "                    WHEN COUNT(tx_prev_month_all_prev_year)>0 THEN SUM(tx_prev_month_all_prev_year.tx_amount)\n",
    "                    ELSE NULL\n",
    "                END AS tx_prev_month_prev_year_total_amount, \n",
    "\n",
    "                CASE \n",
    "                    WHEN  COUNT(tx_prev_month_all_prev_year)>0 THEN COUNT(tx_prev_month_all_prev_year)\n",
    "                    ELSE NULL\n",
    "                END AS tx_prev_month_prev_year_montly_freq\n",
    "            WITH\n",
    "            first_of_previous_month,\n",
    "            c, \n",
    "            AVG(tx_prev_month_prev_year_total_amount) AS tx_prev_month_all_prev_year_total_amount_avg, \n",
    "            AVG(tx_prev_month_prev_year_montly_freq) AS tx_prev_month_all_prev_year_montly_freq_avg\n",
    "\n",
    "            OPTIONAL MATCH (c)-[tx:Make_transaction]->(:Terminal)\n",
    "            WHERE \n",
    "                tx.tx_date_month = first_of_previous_month.month AND \n",
    "                tx.tx_date_year = first_of_previous_month.year\n",
    "            WITH\n",
    "                c,\n",
    "                SUM(tx.tx_amount) AS total_amount_prev_month, \n",
    "                COUNT(tx) AS monthly_freq_prev_month,\n",
    "                tx_prev_month_all_prev_year_total_amount_avg,\n",
    "                tx_prev_month_all_prev_year_montly_freq_avg\n",
    "\n",
    "            RETURN\n",
    "                c,\n",
    "\n",
    "                CASE \n",
    "                    WHEN tx_prev_month_all_prev_year_total_amount_avg IS NULL THEN NULL\n",
    "                    ELSE total_amount_prev_month < tx_prev_month_all_prev_year_total_amount_avg\n",
    "                END AS is_under_total_amount_avg_of_same_period,\n",
    "\n",
    "                CASE \n",
    "                    WHEN tx_prev_month_all_prev_year_montly_freq_avg IS NULL THEN NULL\n",
    "                    ELSE monthly_freq_prev_month < tx_prev_month_all_prev_year_montly_freq_avg\n",
    "                END AS is_under_monthly_freq_avg_of_same_period\n",
    "    \"\"\"\n",
    "\n",
    "    return execute_query_df(\"query_a1\",query)\n",
    "\n",
    "query_a1(\"2023-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63752-1b6a-45d7-a369-d4105a3b2af5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
